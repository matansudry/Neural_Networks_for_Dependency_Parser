{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src import utils\n",
    "from src import model2 as m2\n",
    "import src.dataset as dset\n",
    "import src.pytorch_utils as ptu\n",
    "import src.chu_liu_edmonds as chu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "versions_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dset.DataSet('data/train.labeled', tqdm_bar=True, use_glove=True)\n",
    "# test_dataset = dset.DataSet('data/test.labeled', train_dataset=train_dataset, tqdm_bar=True, use_glove=True)\n",
    "# comp_dataset = dset.DataSet('data/comp.unlabeled', train_dataset=train_dataset, tagged=False, use_glove=True, tqdm_bar=True)\n",
    "\n",
    "# with open(os.path.join('data', 'train_dataset.pth'), 'wb') as f:\n",
    "#     dill.dump(train_dataset, f)\n",
    "# with open(os.path.join('data', 'test_dataset.pth'), 'wb') as f:\n",
    "#     dill.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train_dataset.pth'), \"rb\") as f:\n",
    "    train_dataset = dill.load(f)\n",
    "with open(os.path.join('data', 'test_dataset.pth'), \"rb\") as f:\n",
    "    test_dataset = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'V2_1.10'\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: V2_1.10\n",
      "Number of parameters 8155736 trainable 3905936\n"
     ]
    }
   ],
   "source": [
    "model = m2.Model2(train_dataset=train_dataset,\n",
    "                  word_embed_dim=300,\n",
    "                  tag_embed_dim=32,\n",
    "                  hidden_dim=200,\n",
    "                  num_layers=4,\n",
    "                  bias=True,\n",
    "                  attention_dim=200,\n",
    "                  lstm_activation=None,\n",
    "#                   attn_activation=nn.Tanh(),\n",
    "                  p_dropout=0.4,  # 0.5\n",
    "                  attention=utils.MultiplicativeAttention,\n",
    "#                   softmax=nn.Softmax(dim=2),\n",
    "                  softmax=nn.LogSoftmax(dim=2),\n",
    "                  glove=True,\n",
    "                  freeze=True)\n",
    "\n",
    "checkpoint = ptu.Checkpoint(versions_dir=versions_dir,\n",
    "                            version=version,\n",
    "                            model=model,\n",
    "                            score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                            loss_decision_func=utils.loss_decision_func,\n",
    "#                             out_decision_func=lambda y_pred, flat_y_pred, mask, padding: flat_y_pred.argmax(axis=1),\n",
    "                            out_decision_func=chu.test_chu_liu_edmonds,\n",
    "                            seed=42,\n",
    "#                             optimizer=QHAdam,\n",
    "                            optimizer=torch.optim.Adam,\n",
    "#                             optimizer=torch.optim.AdamW,\n",
    "                            criterion=nn.NLLLoss,\n",
    "#                             criterion=nn.MSELoss,\n",
    "                            save=save,\n",
    "                            prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ptu.load_model(version=version, versions_dir=versions_dir, epoch=40, seed=42)\n",
    "# display(checkpoint.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 20 | train_loss 0.45094 | val_loss 0.48463 | train_score 0.86644 | val_score 0.85714 | train_time   1.02 min *\n",
      "epoch   2/ 20 | train_loss 0.32350 | val_loss 0.40231 | train_score 0.89766 | val_score 0.87692 | train_time   2.60 min *\n",
      "epoch   3/ 20 | train_loss 0.25138 | val_loss 0.37215 | train_score 0.91963 | val_score 0.88576 | train_time   3.90 min *\n",
      "epoch   4/ 20 | train_loss 0.20477 | val_loss 0.37632 | train_score 0.93485 | val_score 0.88785 | train_time   5.22 min *\n",
      "epoch   5/ 20 | train_loss 0.16689 | val_loss 0.37997 | train_score 0.94613 | val_score 0.89159 | train_time   6.43 min *\n",
      "epoch   6/ 20 | train_loss 0.14055 | val_loss 0.40346 | train_score 0.95381 | val_score 0.89118 | train_time   7.90 min\n",
      "epoch   7/ 20 | train_loss 0.11738 | val_loss 0.40196 | train_score 0.96157 | val_score 0.90195 | train_time   9.00 min *\n",
      "epoch   8/ 20 | train_loss 0.10202 | val_loss 0.40818 | train_score 0.96722 | val_score 0.89517 | train_time  10.39 min\n",
      "epoch   9/ 20 | train_loss 0.08435 | val_loss 0.42875 | train_score 0.97249 | val_score 0.89805 | train_time  11.48 min\n",
      "epoch  10/ 20 | train_loss 0.07259 | val_loss 0.41488 | train_score 0.97626 | val_score 0.90347 | train_time  12.56 min *\n",
      "epoch  11/ 20 | train_loss 0.06336 | val_loss 0.46382 | train_score 0.97803 | val_score 0.90199 | train_time  14.05 min\n",
      "epoch  12/ 20 | train_loss 0.06038 | val_loss 0.45064 | train_score 0.98022 | val_score 0.90134 | train_time  15.16 min\n",
      "epoch  13/ 20 | train_loss 0.04488 | val_loss 0.45836 | train_score 0.98484 | val_score 0.90800 | train_time  16.37 min *\n",
      "epoch  14/ 20 | train_loss 0.04689 | val_loss 0.49351 | train_score 0.98482 | val_score 0.90269 | train_time  17.81 min\n",
      "epoch  15/ 20 | train_loss 0.04570 | val_loss 0.49218 | train_score 0.98467 | val_score 0.90450 | train_time  18.93 min\n",
      "epoch  16/ 20 | train_loss 0.04110 | val_loss 0.51372 | train_score 0.98686 | val_score 0.90162 | train_time  20.22 min\n",
      "epoch  17/ 20 | train_loss 0.04039 | val_loss 0.49561 | train_score 0.98710 | val_score 0.90590 | train_time  21.30 min\n",
      "epoch  18/ 20 | train_loss 0.03420 | val_loss 0.51171 | train_score 0.98887 | val_score 0.90656 | train_time  22.39 min\n",
      "epoch  19/ 20 | train_loss 0.03299 | val_loss 0.51967 | train_score 0.98908 | val_score 0.90541 | train_time  23.64 min\n",
      "epoch  20/ 20 | train_loss 0.03142 | val_loss 0.50892 | train_score 0.98941 | val_score 0.90746 | train_time  24.73 min\n",
      "epoch  21/ 40 | train_loss 0.02951 | val_loss 0.50682 | train_score 0.99022 | val_score 0.90779 | train_time  25.71 min\n",
      "epoch  22/ 40 | train_loss 0.01848 | val_loss 0.54653 | train_score 0.99353 | val_score 0.90804 | train_time  26.78 min *\n",
      "epoch  23/ 40 | train_loss 0.01306 | val_loss 0.57111 | train_score 0.99581 | val_score 0.91022 | train_time  28.04 min *\n",
      "epoch  24/ 40 | train_loss 0.00907 | val_loss 0.60645 | train_score 0.99706 | val_score 0.91133 | train_time  29.29 min *\n",
      "epoch  25/ 40 | train_loss 0.00620 | val_loss 0.62605 | train_score 0.99789 | val_score 0.91223 | train_time  30.59 min *\n",
      "epoch  26/ 40 | train_loss 0.00586 | val_loss 0.63377 | train_score 0.99789 | val_score 0.91396 | train_time  32.09 min *\n",
      "epoch  27/ 40 | train_loss 0.00431 | val_loss 0.67068 | train_score 0.99834 | val_score 0.91367 | train_time  33.55 min\n",
      "epoch  28/ 40 | train_loss 0.00311 | val_loss 0.68613 | train_score 0.99878 | val_score 0.91383 | train_time  34.63 min\n",
      "epoch  29/ 40 | train_loss 0.00252 | val_loss 0.67882 | train_score 0.99907 | val_score 0.91375 | train_time  35.76 min\n",
      "epoch  30/ 40 | train_loss 0.00226 | val_loss 0.70273 | train_score 0.99919 | val_score 0.91367 | train_time  36.83 min\n",
      "epoch  31/ 40 | train_loss 0.00209 | val_loss 0.71799 | train_score 0.99920 | val_score 0.91416 | train_time  38.09 min *\n",
      "epoch  32/ 40 | train_loss 0.00193 | val_loss 0.72119 | train_score 0.99933 | val_score 0.91396 | train_time  39.38 min\n",
      "epoch  33/ 40 | train_loss 0.00182 | val_loss 0.72259 | train_score 0.99927 | val_score 0.91367 | train_time  40.48 min\n",
      "epoch  34/ 40 | train_loss 0.00176 | val_loss 0.72656 | train_score 0.99931 | val_score 0.91441 | train_time  41.55 min *\n",
      "epoch  35/ 40 | train_loss 0.00170 | val_loss 0.73211 | train_score 0.99927 | val_score 0.91424 | train_time  42.82 min\n",
      "epoch  36/ 40 | train_loss 0.00164 | val_loss 0.73415 | train_score 0.99933 | val_score 0.91441 | train_time  44.08 min\n",
      "epoch  37/ 40 | train_loss 0.00162 | val_loss 0.73999 | train_score 0.99934 | val_score 0.91449 | train_time  45.28 min *\n",
      "epoch  38/ 40 | train_loss 0.00157 | val_loss 0.74433 | train_score 0.99934 | val_score 0.91433 | train_time  46.63 min\n",
      "epoch  39/ 40 | train_loss 0.00153 | val_loss 0.74635 | train_score 0.99934 | val_score 0.91424 | train_time  47.84 min\n",
      "epoch  40/ 40 | train_loss 0.00150 | val_loss 0.74867 | train_score 0.99938 | val_score 0.91429 | train_time  48.92 min\n",
      "epoch  41/ 60 | train_loss 0.00990 | val_loss 0.64916 | train_score 0.99701 | val_score 0.90923 | train_time  49.94 min\n",
      "epoch  42/ 60 | train_loss 0.00500 | val_loss 0.66210 | train_score 0.99834 | val_score 0.91120 | train_time  51.02 min\n",
      "epoch  43/ 60 | train_loss 0.00367 | val_loss 0.69583 | train_score 0.99884 | val_score 0.91186 | train_time  52.15 min\n",
      "epoch  44/ 60 | train_loss 0.00272 | val_loss 0.72444 | train_score 0.99902 | val_score 0.91301 | train_time  53.31 min\n",
      "epoch  45/ 60 | train_loss 0.00205 | val_loss 0.73705 | train_score 0.99930 | val_score 0.91313 | train_time  54.57 min\n",
      "epoch  46/ 60 | train_loss 0.00173 | val_loss 0.75703 | train_score 0.99946 | val_score 0.91239 | train_time  55.89 min\n",
      "epoch  47/ 60 | train_loss 0.00146 | val_loss 0.75575 | train_score 0.99955 | val_score 0.91215 | train_time  56.98 min\n",
      "epoch  48/ 60 | train_loss 0.00139 | val_loss 0.76721 | train_score 0.99953 | val_score 0.91276 | train_time  58.04 min\n",
      "epoch  49/ 60 | train_loss 0.00122 | val_loss 0.78060 | train_score 0.99962 | val_score 0.91239 | train_time  59.13 min\n",
      "epoch  50/ 60 | train_loss 0.00111 | val_loss 0.77915 | train_score 0.99963 | val_score 0.91350 | train_time  60.21 min\n",
      "epoch  51/ 60 | train_loss 0.00102 | val_loss 0.78776 | train_score 0.99967 | val_score 0.91404 | train_time  61.44 min\n",
      "epoch  52/ 60 | train_loss 0.00098 | val_loss 0.79350 | train_score 0.99971 | val_score 0.91371 | train_time  62.55 min\n",
      "epoch  53/ 60 | train_loss 0.00098 | val_loss 0.79815 | train_score 0.99969 | val_score 0.91416 | train_time  63.81 min\n",
      "epoch  54/ 60 | train_loss 0.00094 | val_loss 0.80307 | train_score 0.99971 | val_score 0.91396 | train_time  65.01 min\n",
      "epoch  55/ 60 | train_loss 0.00090 | val_loss 0.80774 | train_score 0.99976 | val_score 0.91404 | train_time  66.09 min\n",
      "epoch  56/ 60 | train_loss 0.00088 | val_loss 0.80879 | train_score 0.99975 | val_score 0.91404 | train_time  67.34 min\n",
      "epoch  57/ 60 | train_loss 0.00087 | val_loss 0.80895 | train_score 0.99971 | val_score 0.91383 | train_time  68.41 min\n",
      "epoch  58/ 60 | train_loss 0.00087 | val_loss 0.80923 | train_score 0.99971 | val_score 0.91404 | train_time  69.58 min\n",
      "epoch  59/ 60 | train_loss 0.00085 | val_loss 0.81023 | train_score 0.99972 | val_score 0.91429 | train_time  70.65 min\n",
      "epoch  60/ 60 | train_loss 0.00085 | val_loss 0.81114 | train_score 0.99971 | val_score 0.91441 | train_time  71.73 min\n"
     ]
    }
   ],
   "source": [
    "word_dropout_alpha = 0.25\n",
    "hyperparam_list = [\n",
    "    {'train_epochs': 20, 'batch_size': 8, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 5e-7}},\n",
    "    {'train_epochs': 20, 'batch_size': 8, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1e-6}, 'lr_decay': 0.2},\n",
    "#     {'train_epochs': 15, 'batch_size': 16, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 0.0}},  # 5e-7\n",
    "    {'train_epochs': 20, 'batch_size': 8, 'optimizer_params': {'lr': 1e-3, 'weight_decay': 0.0}, 'lr_decay': 0.2},  # 1e-6\n",
    "#     {'train_epochs': 30, 'batch_size': 32, 'optimizer_params': {}},  # 1e-6\n",
    "#     {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1e-5}, 'lr_decay': 0.2},\n",
    "#     {'train_epochs': 20, 'batch_size': 32, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1.5e-6}, 'lr_decay': 0.2},\n",
    "#     {'train_epochs': 20, 'batch_size': 64, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1.5e-6}, 'lr_decay': 0.2},\n",
    "]\n",
    "\n",
    "for session in hyperparam_list:\n",
    "    checkpoint.train(device=device,\n",
    "                     train_dataset=train_dataset.dataset(word_dropout_alpha, train=True),\n",
    "                     val_dataset=test_dataset.dataset(train=False),\n",
    "                     prints=True,\n",
    "                     epochs_save=5,\n",
    "                     save=save,\n",
    "#                      early_stop=5,\n",
    "                     **session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_numpy = \n",
    "estimated_tree_heads = np.zeros((batch_size, max_sentence_len))\n",
    "for b in range(batch_size):\n",
    "    # sentence len is the unpadded sentence length - important when we add batches\n",
    "    estimated_tree_heads[b, :], _ = self.decoder(scores_numpy[b, :, :], unsorted_sentences_lengths[b], has_labels=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlabeled_attachment_score(scores, heads, lengths):\n",
    "    uas = 0\n",
    "    for batch, length in enumerate(lengths):\n",
    "        length = length.item()\n",
    "        parse_tree, _ = decode_mst(scores[batch, :, :].detach().cpu().numpy(), length, has_labels=False)\n",
    "        uas += sum(parse_tree[i] == heads[batch, i].item() for i in range(length)) / length\n",
    "    return uas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chu_li(y_pred, flat_y_pred, mask, padding):\n",
    "    lengths = mask.sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'V2_hpo_1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = {\n",
    "    'Additive': utils.AdditiveAttention,\n",
    "    'Multiplicative': utils.MultiplicativeAttention,\n",
    "}\n",
    "\n",
    "softmaxs = {\n",
    "    'LogSoftmax': nn.LogSoftmax(dim=2),\n",
    "#     'Softmax': nn.Softmax(dim=2),\n",
    "}\n",
    "\n",
    "activations = dict(sorted(list({\n",
    "    'tanh': nn.Tanh(),\n",
    "    'hard_tanh': nn.Hardtanh(),\n",
    "#     'relu': nn.ReLU(),\n",
    "#     'elu': nn.ELU(),\n",
    "#     'leaky_relu': nn.LeakyReLU(),\n",
    "    'p_relu': nn.PReLU(),\n",
    "#     'relu6': nn.ReLU6(),\n",
    "#     'gelu': nn.GELU(),\n",
    "#     'sigmoid': nn.Sigmoid(),\n",
    "}.items()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt as hpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_space = dict(sorted(list({\n",
    "#     'train_epochs': 50,\n",
    "    'batch_size': 16, #hpo.hp.quniform('batch_size', low=4, high=5, q=1),  # 16-32-64\n",
    "    'optimizer__lr': hpo.hp.uniform('optimizer__lr', low=8e-4, high=2e-3),\n",
    "    'optimizer__wd': hpo.hp.uniform('optimizer__wd', low=5e-7, high=5e-6),# 0.0\n",
    "#     'early_stop': 5,\n",
    "    \n",
    "    'word_embed_dim': 300,  # 300\n",
    "    'tag_embed_dim': 32, #hpo.hp.quniform('tag_embed_dim', low=30, high=50, q=4), #25\n",
    "    'hidden_dim': hpo.hp.quniform('hidden_dim', low=200, high=300, q=50), #125,  # \n",
    "    'num_layers': hpo.hp.quniform('num_layers', low=3, high=4, q=1),#2,  # \n",
    "    'bias': True, #hpo.hp.choice('bias', [True, False]),\n",
    "    'attention_dim': hpo.hp.quniform('attention_dim', low=200, high=300, q=50),#100,  # \n",
    "    'attention': hpo.hp.choice('attention', list(attentions.keys())),\n",
    "    'activation': hpo.hp.choice('activation', list(activations.keys())),\n",
    "    'softmax': hpo.hp.choice('softmax', list(softmaxs.keys())),\n",
    "    'p_dropout': hpo.hp.uniform('p_dropout', low=0.3, high=0.6),#0.1,  # \n",
    "    'lr_decay': hpo.hp.uniform('lr_decay', low=0.15, high=0.25),#0.1,  # \n",
    "    'freeze': True, #hpo.hp.choice('freeze', [True, False]),\n",
    "}.items()), key=lambda x: x[0]))\n",
    "\n",
    "def init_objective(space, save=False):\n",
    "    display(space)\n",
    "    last_score = init_log['test_score'].max() if len(init_log) > 0 else 0.0\n",
    "    batch_size = int(2 ** space['batch_size'])\n",
    "#     attention = utils.MultiplicativeAttention if space['attention'] == 'Multiplicative' else utils.AdditiveAttention\n",
    "#     activation = space['attention'] if space['attention'] != 'Multiplicative' else 'tanh'\n",
    "#     activation = activations[activation]\n",
    "    \n",
    "    model = m2.Model2(train_dataset=train_dataset,\n",
    "                      word_embed_dim=space['word_embed_dim'],  # 300\n",
    "                      tag_embed_dim=space['tag_embed_dim'],  # 32\n",
    "                      hidden_dim=int(space['hidden_dim']),  # 125\n",
    "                      num_layers=int(space['num_layers']),  # 2\n",
    "                      bias=space['bias'],  # True\n",
    "                      attention_dim=int(space['attention_dim']),  # 10\n",
    "                      activation=activations[space['activation']],\n",
    "                      p_dropout=space['p_dropout'],  # 0.5\n",
    "                      attention=attentions[space['attention']],\n",
    "                      softmax=softmaxs[space['softmax']],\n",
    "                      glove=True,\n",
    "                      freeze=space['freeze'])\n",
    "\n",
    "    init_checkpoint = ptu.Checkpoint(versions_dir=versions_dir,\n",
    "                                     version=version,\n",
    "                                     model=model,\n",
    "                                     score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                                     loss_decision_func=utils.loss_decision_func,\n",
    "                                     out_decision_func=lambda y_pred, flat_y_pred, mask, padding: flat_y_pred.argmax(axis=1),\n",
    "                                     seed=42,\n",
    "                                     optimizer=torch.optim.AdamW,\n",
    "                                     criterion=nn.NLLLoss,\n",
    "                                     save=False,\n",
    "                                     prints=True)\n",
    "    \n",
    "    word_dropout_alpha = 0.25\n",
    "    hyperparam_list = [\n",
    "#         {'train_epochs': 1, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': 5e-7}},\n",
    "        {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': 5e-7}},\n",
    "        {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': space['optimizer__wd']}, 'lr_decay': space['lr_decay']},\n",
    "#         {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': space['optimizer__wd']}, 'lr_decay': space['lr_decay']},\n",
    "    #     {'train_epochs': 20, 'batch_size': 32, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1.5e-6}, 'lr_decay': 0.2},\n",
    "    #     {'train_epochs': 20, 'batch_size': 64, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1.5e-6}, 'lr_decay': 0.2},\n",
    "    ]\n",
    "\n",
    "    for session in hyperparam_list:\n",
    "        init_checkpoint.train(device=device,\n",
    "                              train_dataset=train_dataset.dataset(word_dropout_alpha, train=True),\n",
    "                              val_dataset=test_dataset.dataset(train=False),\n",
    "                              prints=True,\n",
    "                              epochs_save=5,\n",
    "                              save=save,\n",
    "        #                       early_stop=5,\n",
    "                              **session)    \n",
    "    \n",
    "    train_score = init_checkpoint.get_log(col='train_score', epoch=-1)\n",
    "    test_score = init_checkpoint.get_log(col='val_score', epoch=-1)\n",
    "#     print('test_score', test_score)\n",
    "    ###############################################################\n",
    "    if test_score > last_score:\n",
    "        init_checkpoint.save(epoch=True)\n",
    "    init_log.loc[init_log.index.max() + 1 if len(init_log) > 0 else 0] = [time.strftime('%d-%m-%Y %H:%M:%S'),\n",
    "#                                                                           train_score,\n",
    "                                                                          test_score,\n",
    "                                                                          space] + list(space.values())\n",
    "    \n",
    "    with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "        dill.dump(init_trials, f)\n",
    "    init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)\n",
    "\n",
    "    return -test_score\n",
    "\n",
    "# session_space = dict(sorted(list({\n",
    "#     'train_epochs': 5,\n",
    "#     'batch_size_mult': min(len(X_train), int(2**hpo.hp.quniform('batch_size_mult', low=5, high=9, q=1))),\n",
    "#     'optimizer__lr_mult': hpo.hp.uniform('optimizer__lr_mult', low=1e-5, high=1e-3),\n",
    "#     'optimizer__weight_decay': hpo.hp.uniform('optimizer__weight_decay', low=1e-5, high=1e-3),\n",
    "#     'p_dropout': max(0.0, min(0.9, hpo.hp.normal('p_dropout', mu=0.5, sigma=0.15))),\n",
    "# }.items()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>test_score</th>\n",
       "      <th>space</th>\n",
       "      <th>activation</th>\n",
       "      <th>attention</th>\n",
       "      <th>attention_dim</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>bias</th>\n",
       "      <th>freeze</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>lr_decay</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>optimizer__lr</th>\n",
       "      <th>optimizer__wd</th>\n",
       "      <th>p_dropout</th>\n",
       "      <th>softmax</th>\n",
       "      <th>tag_embed_dim</th>\n",
       "      <th>word_embed_dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27-06-2020 00:29:23</td>\n",
       "      <td>0.907996</td>\n",
       "      <td>{'activation': 'tanh', 'attention': 'Multiplic...</td>\n",
       "      <td>tanh</td>\n",
       "      <td>Multiplicative</td>\n",
       "      <td>250.0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.182835</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>4.488547e-06</td>\n",
       "      <td>0.327025</td>\n",
       "      <td>LogSoftmax</td>\n",
       "      <td>32</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27-06-2020 01:23:47</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>{'activation': 'hard_tanh', 'attention': 'Addi...</td>\n",
       "      <td>hard_tanh</td>\n",
       "      <td>Additive</td>\n",
       "      <td>250.0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.160718</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>6.287681e-07</td>\n",
       "      <td>0.400194</td>\n",
       "      <td>LogSoftmax</td>\n",
       "      <td>32</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27-06-2020 01:38:03</td>\n",
       "      <td>0.905612</td>\n",
       "      <td>{'activation': 'tanh', 'attention': 'Multiplic...</td>\n",
       "      <td>tanh</td>\n",
       "      <td>Multiplicative</td>\n",
       "      <td>250.0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.232751</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>2.992325e-06</td>\n",
       "      <td>0.431738</td>\n",
       "      <td>LogSoftmax</td>\n",
       "      <td>32</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27-06-2020 01:49:03</td>\n",
       "      <td>0.911531</td>\n",
       "      <td>{'activation': 'p_relu', 'attention': 'Multipl...</td>\n",
       "      <td>p_relu</td>\n",
       "      <td>Multiplicative</td>\n",
       "      <td>250.0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.165322</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>3.360282e-06</td>\n",
       "      <td>0.515076</td>\n",
       "      <td>LogSoftmax</td>\n",
       "      <td>32</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27-06-2020 02:45:22</td>\n",
       "      <td>0.895005</td>\n",
       "      <td>{'activation': 'tanh', 'attention': 'Additive'...</td>\n",
       "      <td>tanh</td>\n",
       "      <td>Additive</td>\n",
       "      <td>250.0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.216024</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>4.940200e-06</td>\n",
       "      <td>0.475179</td>\n",
       "      <td>LogSoftmax</td>\n",
       "      <td>32</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27-06-2020 03:33:44</td>\n",
       "      <td>0.897842</td>\n",
       "      <td>{'activation': 'tanh', 'attention': 'Additive'...</td>\n",
       "      <td>tanh</td>\n",
       "      <td>Additive</td>\n",
       "      <td>200.0</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>2.365344e-06</td>\n",
       "      <td>0.426961</td>\n",
       "      <td>LogSoftmax</td>\n",
       "      <td>32</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  test_score  \\\n",
       "0  27-06-2020 00:29:23    0.907996   \n",
       "1  27-06-2020 01:23:47    0.905858   \n",
       "2  27-06-2020 01:38:03    0.905612   \n",
       "3  27-06-2020 01:49:03    0.911531   \n",
       "4  27-06-2020 02:45:22    0.895005   \n",
       "5  27-06-2020 03:33:44    0.897842   \n",
       "\n",
       "                                               space activation  \\\n",
       "0  {'activation': 'tanh', 'attention': 'Multiplic...       tanh   \n",
       "1  {'activation': 'hard_tanh', 'attention': 'Addi...  hard_tanh   \n",
       "2  {'activation': 'tanh', 'attention': 'Multiplic...       tanh   \n",
       "3  {'activation': 'p_relu', 'attention': 'Multipl...     p_relu   \n",
       "4  {'activation': 'tanh', 'attention': 'Additive'...       tanh   \n",
       "5  {'activation': 'tanh', 'attention': 'Additive'...       tanh   \n",
       "\n",
       "        attention  attention_dim  batch_size  bias  freeze  hidden_dim  \\\n",
       "0  Multiplicative          250.0          16  True   False       250.0   \n",
       "1        Additive          250.0          16  True    True       200.0   \n",
       "2  Multiplicative          250.0          16  True   False       200.0   \n",
       "3  Multiplicative          250.0          16  True    True       200.0   \n",
       "4        Additive          250.0          16  True   False       250.0   \n",
       "5        Additive          200.0          16  True    True       200.0   \n",
       "\n",
       "   lr_decay  num_layers  optimizer__lr  optimizer__wd  p_dropout     softmax  \\\n",
       "0  0.182835         4.0       0.001348   4.488547e-06   0.327025  LogSoftmax   \n",
       "1  0.160718         3.0       0.001237   6.287681e-07   0.400194  LogSoftmax   \n",
       "2  0.232751         4.0       0.001468   2.992325e-06   0.431738  LogSoftmax   \n",
       "3  0.165322         3.0       0.001895   3.360282e-06   0.515076  LogSoftmax   \n",
       "4  0.216024         3.0       0.001702   4.940200e-06   0.475179  LogSoftmax   \n",
       "5  0.170843         4.0       0.001632   2.365344e-06   0.426961  LogSoftmax   \n",
       "\n",
       "   tag_embed_dim  word_embed_dim  \n",
       "0             32             300  \n",
       "1             32             300  \n",
       "2             32             300  \n",
       "3             32             300  \n",
       "4             32             300  \n",
       "5             32             300  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init_trials = hpo.Trials()\n",
    "# init_log = pd.DataFrame(columns=['timestamp',\n",
    "#                                  # 'train_score',\n",
    "#                                  'test_score',\n",
    "#                                  'space'] + list(init_space.keys()))\n",
    "\n",
    "# with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "#     dill.dump(init_trials, f)\n",
    "# init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(versions_dir, version, 'trials.pth'), \"rb\") as f:\n",
    "    init_trials = dill.load(f)\n",
    "init_log = pd.read_csv(os.path.join(versions_dir, version, 'trials_log.csv'))\n",
    "display(init_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'hard_tanh',\n",
       " 'attention': 'Additive',\n",
       " 'attention_dim': 250.0,\n",
       " 'batch_size': 16,\n",
       " 'bias': True,\n",
       " 'freeze': False,\n",
       " 'hidden_dim': 300.0,\n",
       " 'lr_decay': 0.18403316927816465,\n",
       " 'num_layers': 3.0,\n",
       " 'optimizer__lr': 0.0010580590955336128,\n",
       " 'optimizer__wd': 3.0539062944273563e-06,\n",
       " 'p_dropout': 0.542777185183592,\n",
       " 'softmax': 'LogSoftmax',\n",
       " 'tag_embed_dim': 32,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                         \n",
      "V2_hpo_1.0                                                     \n",
      "Number of parameters 10403287 trainable 10403287               \n",
      "epoch   1/ 20 | train_loss 0.64693 | val_loss 0.67034 | train_score 0.81441 | val_score 0.81052 | train_time   1.46 min *\n",
      "epoch   2/ 20 | train_loss 0.41799 | val_loss 0.49972 | train_score 0.87624 | val_score 0.85427 | train_time   2.91 min *\n",
      "epoch   3/ 20 | train_loss 0.30594 | val_loss 0.44829 | train_score 0.90902 | val_score 0.86643 | train_time   4.37 min *\n",
      "epoch   4/ 20 | train_loss 0.23014 | val_loss 0.43653 | train_score 0.93058 | val_score 0.87404 | train_time   5.82 min *\n",
      "epoch   5/ 20 | train_loss 0.18785 | val_loss 0.45082 | train_score 0.94163 | val_score 0.87963 | train_time   7.28 min *\n",
      "epoch   6/ 20 | train_loss 0.15673 | val_loss 0.48232 | train_score 0.95123 | val_score 0.87437 | train_time   8.73 min\n",
      "epoch   7/ 20 | train_loss 0.12603 | val_loss 0.49341 | train_score 0.96168 | val_score 0.87733 | train_time  10.19 min\n",
      "epoch   8/ 20 | train_loss 0.11180 | val_loss 0.48784 | train_score 0.96589 | val_score 0.88234 | train_time  11.64 min *\n",
      "epoch   9/ 20 | train_loss 0.08201 | val_loss 0.50562 | train_score 0.97533 | val_score 0.88645 | train_time  13.10 min *\n",
      "epoch  10/ 20 | train_loss 0.07039 | val_loss 0.53770 | train_score 0.97792 | val_score 0.88259 | train_time  14.55 min\n",
      "epoch  11/ 20 | train_loss 0.06668 | val_loss 0.58418 | train_score 0.97849 | val_score 0.88288 | train_time  16.01 min\n",
      "epoch  12/ 20 | train_loss 0.06157 | val_loss 0.57478 | train_score 0.98050 | val_score 0.87938 | train_time  17.46 min\n",
      "epoch  13/ 20 | train_loss 0.04787 | val_loss 0.61473 | train_score 0.98519 | val_score 0.88247 | train_time  18.92 min\n",
      "epoch  14/ 20 | train_loss 0.04103 | val_loss 0.56088 | train_score 0.98676 | val_score 0.88974 | train_time  20.37 min *\n",
      "epoch  15/ 20 | train_loss 0.03636 | val_loss 0.60671 | train_score 0.98858 | val_score 0.88995 | train_time  21.82 min *\n",
      "epoch  16/ 20 | train_loss 0.03010 | val_loss 0.60879 | train_score 0.99095 | val_score 0.89205 | train_time  23.28 min *\n",
      "epoch  17/ 20 | train_loss 0.02889 | val_loss 0.66488 | train_score 0.99081 | val_score 0.88732 | train_time  24.73 min\n",
      "epoch  18/ 20 | train_loss 0.02867 | val_loss 0.67536 | train_score 0.99050 | val_score 0.88419 | train_time  26.19 min\n",
      "epoch  19/ 20 | train_loss 0.02618 | val_loss 0.67797 | train_score 0.99166 | val_score 0.88530 | train_time  27.64 min\n",
      "epoch  20/ 20 | train_loss 0.02370 | val_loss 0.66359 | train_score 0.99229 | val_score 0.88966 | train_time  29.09 min\n",
      "epoch  21/ 40 | train_loss 0.02192 | val_loss 0.70450 | train_score 0.99291 | val_score 0.89073 | train_time  30.55 min\n",
      "epoch  22/ 40 | train_loss 0.01307 | val_loss 0.73329 | train_score 0.99623 | val_score 0.89340 | train_time  32.00 min *\n",
      "epoch  23/ 40 | train_loss 0.00748 | val_loss 0.74284 | train_score 0.99771 | val_score 0.89369 | train_time  33.45 min *\n",
      "epoch  24/ 40 | train_loss 0.00488 | val_loss 0.74896 | train_score 0.99856 | val_score 0.89612 | train_time  34.91 min *\n",
      "epoch  25/ 40 | train_loss 0.00354 | val_loss 0.77288 | train_score 0.99890 | val_score 0.89513 | train_time  36.36 min\n",
      "epoch  26/ 40 | train_loss 0.00243 | val_loss 0.80567 | train_score 0.99924 | val_score 0.89612 | train_time  37.81 min\n",
      "epoch  27/ 40 | train_loss 0.00206 | val_loss 0.81252 | train_score 0.99934 | val_score 0.89669 | train_time  39.27 min *\n",
      "epoch  28/ 40 | train_loss 0.00165 | val_loss 0.82967 | train_score 0.99950 | val_score 0.89579 | train_time  40.72 min\n",
      "epoch  29/ 40 | train_loss 0.00149 | val_loss 0.83124 | train_score 0.99949 | val_score 0.89653 | train_time  42.17 min\n",
      "epoch  30/ 40 | train_loss 0.00125 | val_loss 0.86094 | train_score 0.99960 | val_score 0.89628 | train_time  43.63 min\n",
      "epoch  31/ 40 | train_loss 0.00111 | val_loss 0.85639 | train_score 0.99965 | val_score 0.89620 | train_time  45.08 min\n",
      "epoch  32/ 40 | train_loss 0.00100 | val_loss 0.86014 | train_score 0.99971 | val_score 0.89657 | train_time  46.53 min\n",
      "epoch  33/ 40 | train_loss 0.00090 | val_loss 0.87211 | train_score 0.99972 | val_score 0.89710 | train_time  47.99 min *\n",
      "epoch  34/ 40 | train_loss 0.00084 | val_loss 0.86934 | train_score 0.99973 | val_score 0.89677 | train_time  49.44 min\n",
      "epoch  35/ 40 | train_loss 0.00078 | val_loss 0.87234 | train_score 0.99978 | val_score 0.89772 | train_time  50.89 min *\n",
      "epoch  36/ 40 | train_loss 0.00075 | val_loss 0.88009 | train_score 0.99977 | val_score 0.89706 | train_time  52.35 min\n",
      "epoch  37/ 40 | train_loss 0.00073 | val_loss 0.88102 | train_score 0.99979 | val_score 0.89784 | train_time  53.80 min *\n",
      "epoch  38/ 40 | train_loss 0.00073 | val_loss 0.88214 | train_score 0.99979 | val_score 0.89710 | train_time  55.25 min\n",
      "epoch  39/ 40 | train_loss 0.00072 | val_loss 0.88106 | train_score 0.99976 | val_score 0.89718 | train_time  56.71 min\n",
      "epoch  40/ 40 | train_loss 0.00070 | val_loss 0.88672 | train_score 0.99979 | val_score 0.89735 | train_time  58.16 min\n",
      "  1%|          | 6/500 [58:11<479:07:19, 3491.58s/trial, best loss: -0.9115313463514902]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'attention': 'Multiplicative',\n",
       " 'attention_dim': 300.0,\n",
       " 'batch_size': 16,\n",
       " 'bias': True,\n",
       " 'freeze': False,\n",
       " 'hidden_dim': 250.0,\n",
       " 'lr_decay': 0.20709048937348762,\n",
       " 'num_layers': 3.0,\n",
       " 'optimizer__lr': 0.0009120247387124067,\n",
       " 'optimizer__wd': 2.718449748748443e-06,\n",
       " 'p_dropout': 0.45152728232954664,\n",
       " 'softmax': 'LogSoftmax',\n",
       " 'tag_embed_dim': 32,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                          \n",
      "V2_hpo_1.0                                                                              \n",
      "Number of parameters 8677836 trainable 8677836                                          \n",
      "epoch   1/ 20 | train_loss 0.56802 | val_loss 0.60253 | train_score 0.83325 | val_score 0.82565 | train_time   0.32 min *\n",
      "epoch   2/ 20 | train_loss 0.34190 | val_loss 0.45287 | train_score 0.89566 | val_score 0.86557 | train_time   0.65 min *\n",
      "epoch   3/ 20 | train_loss 0.23812 | val_loss 0.42762 | train_score 0.92393 | val_score 0.87572 | train_time   0.97 min *\n",
      "epoch   4/ 20 | train_loss 0.17725 | val_loss 0.43899 | train_score 0.94189 | val_score 0.87836 | train_time   1.30 min *\n",
      "epoch   5/ 20 | train_loss 0.14049 | val_loss 0.45358 | train_score 0.95399 | val_score 0.88185 | train_time   1.62 min *\n",
      "epoch   6/ 20 | train_loss 0.11512 | val_loss 0.51391 | train_score 0.96068 | val_score 0.88004 | train_time   1.95 min\n",
      "epoch   7/ 20 | train_loss 0.08665 | val_loss 0.54365 | train_score 0.97072 | val_score 0.88251 | train_time   2.27 min *\n",
      "epoch   8/ 20 | train_loss 0.07246 | val_loss 0.57562 | train_score 0.97478 | val_score 0.88329 | train_time   2.60 min *\n",
      "epoch   9/ 20 | train_loss 0.06111 | val_loss 0.57624 | train_score 0.97919 | val_score 0.88835 | train_time   2.92 min *\n",
      "epoch  10/ 20 | train_loss 0.03852 | val_loss 0.58847 | train_score 0.98676 | val_score 0.89180 | train_time   3.25 min *\n",
      "epoch  11/ 20 | train_loss 0.04001 | val_loss 0.64369 | train_score 0.98632 | val_score 0.89229 | train_time   3.57 min *\n",
      "epoch  12/ 20 | train_loss 0.03491 | val_loss 0.69904 | train_score 0.98842 | val_score 0.89287 | train_time   3.90 min *\n",
      "epoch  13/ 20 | train_loss 0.02549 | val_loss 0.68346 | train_score 0.99161 | val_score 0.89501 | train_time   4.22 min *\n",
      "epoch  14/ 20 | train_loss 0.02565 | val_loss 0.72154 | train_score 0.99107 | val_score 0.89566 | train_time   4.55 min *\n",
      "epoch  15/ 20 | train_loss 0.01760 | val_loss 0.72110 | train_score 0.99396 | val_score 0.89673 | train_time   4.87 min *\n",
      "epoch  16/ 20 | train_loss 0.01779 | val_loss 0.75680 | train_score 0.99409 | val_score 0.89751 | train_time   5.20 min *\n",
      "epoch  17/ 20 | train_loss 0.01659 | val_loss 0.73429 | train_score 0.99449 | val_score 0.89764 | train_time   5.52 min *\n",
      "epoch  18/ 20 | train_loss 0.01158 | val_loss 0.78062 | train_score 0.99603 | val_score 0.89772 | train_time   5.85 min *\n",
      "epoch  19/ 20 | train_loss 0.01144 | val_loss 0.81086 | train_score 0.99633 | val_score 0.90084 | train_time   6.17 min *\n",
      "epoch  20/ 20 | train_loss 0.01104 | val_loss 0.76975 | train_score 0.99643 | val_score 0.90195 | train_time   6.50 min *\n",
      "epoch  21/ 40 | train_loss 0.00917 | val_loss 0.83262 | train_score 0.99694 | val_score 0.89805 | train_time   6.82 min\n",
      "epoch  22/ 40 | train_loss 0.00850 | val_loss 0.85585 | train_score 0.99738 | val_score 0.90064 | train_time   7.15 min\n",
      "epoch  23/ 40 | train_loss 0.00301 | val_loss 0.88477 | train_score 0.99904 | val_score 0.90314 | train_time   7.47 min *\n",
      "epoch  24/ 40 | train_loss 0.00191 | val_loss 0.92500 | train_score 0.99940 | val_score 0.90319 | train_time   7.80 min *\n",
      "epoch  25/ 40 | train_loss 0.00135 | val_loss 0.92169 | train_score 0.99954 | val_score 0.90393 | train_time   8.12 min *\n",
      "epoch  26/ 40 | train_loss 0.00108 | val_loss 0.94718 | train_score 0.99963 | val_score 0.90376 | train_time   8.45 min\n",
      "epoch  27/ 40 | train_loss 0.00081 | val_loss 0.95497 | train_score 0.99970 | val_score 0.90458 | train_time   8.77 min *\n",
      "epoch  28/ 40 | train_loss 0.00067 | val_loss 0.96670 | train_score 0.99971 | val_score 0.90413 | train_time   9.10 min\n",
      "epoch  29/ 40 | train_loss 0.00050 | val_loss 0.97273 | train_score 0.99980 | val_score 0.90495 | train_time   9.42 min *\n",
      "epoch  30/ 40 | train_loss 0.00045 | val_loss 0.97997 | train_score 0.99979 | val_score 0.90528 | train_time   9.75 min *\n",
      "epoch  31/ 40 | train_loss 0.00044 | val_loss 0.99295 | train_score 0.99981 | val_score 0.90528 | train_time  10.07 min\n",
      "epoch  32/ 40 | train_loss 0.00039 | val_loss 1.00044 | train_score 0.99982 | val_score 0.90549 | train_time  10.40 min *\n",
      "epoch  33/ 40 | train_loss 0.00036 | val_loss 1.01109 | train_score 0.99986 | val_score 0.90512 | train_time  10.72 min\n",
      "epoch  34/ 40 | train_loss 0.00034 | val_loss 1.01287 | train_score 0.99985 | val_score 0.90569 | train_time  11.05 min *\n",
      "epoch  35/ 40 | train_loss 0.00034 | val_loss 1.02562 | train_score 0.99984 | val_score 0.90561 | train_time  11.37 min\n",
      "epoch  36/ 40 | train_loss 0.00033 | val_loss 1.02927 | train_score 0.99986 | val_score 0.90582 | train_time  11.70 min *\n",
      "epoch  37/ 40 | train_loss 0.00032 | val_loss 1.03344 | train_score 0.99986 | val_score 0.90557 | train_time  12.02 min\n",
      "epoch  38/ 40 | train_loss 0.00030 | val_loss 1.03524 | train_score 0.99987 | val_score 0.90541 | train_time  12.35 min\n",
      "epoch  39/ 40 | train_loss 0.00030 | val_loss 1.04048 | train_score 0.99986 | val_score 0.90569 | train_time  12.67 min\n",
      "epoch  40/ 40 | train_loss 0.00030 | val_loss 1.04310 | train_score 0.99988 | val_score 0.90582 | train_time  13.00 min\n",
      "  1%|â–         | 7/500 [1:11:11<366:46:02, 2678.22s/trial, best loss: -0.9115313463514902]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'p_relu',\n",
       " 'attention': 'Multiplicative',\n",
       " 'attention_dim': 200.0,\n",
       " 'batch_size': 16,\n",
       " 'bias': True,\n",
       " 'freeze': False,\n",
       " 'hidden_dim': 250.0,\n",
       " 'lr_decay': 0.15360017588134772,\n",
       " 'num_layers': 4.0,\n",
       " 'optimizer__lr': 0.0009106221871915555,\n",
       " 'optimizer__wd': 9.831547469304713e-07,\n",
       " 'p_dropout': 0.338099438264459,\n",
       " 'softmax': 'LogSoftmax',\n",
       " 'tag_embed_dim': 32,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                            \n",
      "V2_hpo_1.0                                                                                \n",
      "Number of parameters 10181836 trainable 10181836                                          \n",
      "epoch   1/ 20 | train_loss 0.58668 | val_loss 0.61920 | train_score 0.82878 | val_score 0.82121 | train_time   0.42 min *\n",
      "epoch   2/ 20 | train_loss 0.36370 | val_loss 0.46898 | train_score 0.88808 | val_score 0.85829 | train_time   0.84 min *\n",
      "epoch   3/ 20 | train_loss 0.25032 | val_loss 0.44591 | train_score 0.91955 | val_score 0.86754 | train_time   1.26 min *\n",
      "epoch   4/ 20 | train_loss 0.19306 | val_loss 0.44727 | train_score 0.93754 | val_score 0.87503 | train_time   1.68 min *\n",
      "epoch   5/ 20 | train_loss 0.13996 | val_loss 0.46890 | train_score 0.95357 | val_score 0.88103 | train_time   2.10 min *\n",
      "epoch   6/ 20 | train_loss 0.12647 | val_loss 0.49298 | train_score 0.95868 | val_score 0.88070 | train_time   2.52 min\n",
      "epoch   7/ 20 | train_loss 0.08988 | val_loss 0.54772 | train_score 0.97039 | val_score 0.88255 | train_time   2.94 min *\n",
      "epoch   8/ 20 | train_loss 0.07184 | val_loss 0.57189 | train_score 0.97514 | val_score 0.88629 | train_time   3.36 min *\n",
      "epoch   9/ 20 | train_loss 0.05570 | val_loss 0.57378 | train_score 0.98120 | val_score 0.88748 | train_time   3.78 min *\n",
      "epoch  10/ 20 | train_loss 0.05171 | val_loss 0.64661 | train_score 0.98248 | val_score 0.89028 | train_time   4.20 min *\n",
      "epoch  11/ 20 | train_loss 0.04540 | val_loss 0.65984 | train_score 0.98444 | val_score 0.89065 | train_time   4.62 min *\n",
      "epoch  12/ 20 | train_loss 0.04544 | val_loss 0.69879 | train_score 0.98493 | val_score 0.88687 | train_time   5.04 min\n",
      "epoch  13/ 20 | train_loss 0.03427 | val_loss 0.64175 | train_score 0.98827 | val_score 0.89381 | train_time   5.46 min *\n",
      "epoch  14/ 20 | train_loss 0.02534 | val_loss 0.79648 | train_score 0.99175 | val_score 0.89262 | train_time   5.89 min\n",
      "epoch  15/ 20 | train_loss 0.02288 | val_loss 0.78046 | train_score 0.99271 | val_score 0.89390 | train_time   6.31 min *\n",
      "epoch  16/ 20 | train_loss 0.02240 | val_loss 0.79177 | train_score 0.99254 | val_score 0.89266 | train_time   6.73 min\n",
      "epoch  17/ 20 | train_loss 0.01883 | val_loss 0.78044 | train_score 0.99364 | val_score 0.89205 | train_time   7.15 min\n",
      "epoch  18/ 20 | train_loss 0.01813 | val_loss 0.80134 | train_score 0.99392 | val_score 0.89484 | train_time   7.57 min *\n",
      "epoch  19/ 20 | train_loss 0.02183 | val_loss 0.78965 | train_score 0.99297 | val_score 0.89246 | train_time   7.99 min\n",
      "epoch  20/ 20 | train_loss 0.01234 | val_loss 0.76400 | train_score 0.99594 | val_score 0.89665 | train_time   8.41 min *\n",
      "epoch  21/ 40 | train_loss 0.01371 | val_loss 0.81822 | train_score 0.99573 | val_score 0.89714 | train_time   8.83 min *\n",
      "epoch  22/ 40 | train_loss 0.00730 | val_loss 0.82938 | train_score 0.99791 | val_score 0.90088 | train_time   9.25 min *\n",
      "epoch  23/ 40 | train_loss 0.00562 | val_loss 0.82669 | train_score 0.99826 | val_score 0.90101 | train_time   9.68 min *\n",
      "epoch  24/ 40 | train_loss 0.00272 | val_loss 0.92825 | train_score 0.99910 | val_score 0.90010 | train_time  10.10 min\n",
      "epoch  25/ 40 | train_loss 0.00163 | val_loss 0.95978 | train_score 0.99944 | val_score 0.90084 | train_time  10.53 min\n",
      "epoch  26/ 40 | train_loss 0.00119 | val_loss 0.95851 | train_score 0.99957 | val_score 0.90018 | train_time  10.95 min\n",
      "epoch  27/ 40 | train_loss 0.00085 | val_loss 1.00075 | train_score 0.99970 | val_score 0.90117 | train_time  11.38 min *\n",
      "epoch  28/ 40 | train_loss 0.00076 | val_loss 1.01639 | train_score 0.99976 | val_score 0.90220 | train_time  11.80 min *\n",
      "epoch  29/ 40 | train_loss 0.00074 | val_loss 1.06210 | train_score 0.99976 | val_score 0.90129 | train_time  12.22 min\n",
      "epoch  30/ 40 | train_loss 0.00048 | val_loss 1.05369 | train_score 0.99978 | val_score 0.90134 | train_time  12.64 min\n",
      "epoch  31/ 40 | train_loss 0.00040 | val_loss 1.07967 | train_score 0.99983 | val_score 0.90150 | train_time  13.06 min\n",
      "epoch  32/ 40 | train_loss 0.00037 | val_loss 1.09355 | train_score 0.99981 | val_score 0.90183 | train_time  13.48 min\n",
      "epoch  33/ 40 | train_loss 0.00033 | val_loss 1.11411 | train_score 0.99987 | val_score 0.90179 | train_time  13.91 min\n",
      "epoch  34/ 40 | train_loss 0.00034 | val_loss 1.12912 | train_score 0.99983 | val_score 0.90117 | train_time  14.33 min\n",
      "epoch  35/ 40 | train_loss 0.00029 | val_loss 1.12852 | train_score 0.99986 | val_score 0.90138 | train_time  14.75 min\n",
      "epoch  36/ 40 | train_loss 0.00027 | val_loss 1.14264 | train_score 0.99985 | val_score 0.90138 | train_time  15.17 min\n",
      "epoch  37/ 40 | train_loss 0.00027 | val_loss 1.15596 | train_score 0.99987 | val_score 0.90171 | train_time  15.59 min\n",
      "epoch  38/ 40 | train_loss 0.00026 | val_loss 1.16235 | train_score 0.99986 | val_score 0.90134 | train_time  16.01 min\n",
      "epoch  39/ 40 | train_loss 0.00024 | val_loss 1.16661 | train_score 0.99988 | val_score 0.90166 | train_time  16.44 min\n",
      "epoch  40/ 40 | train_loss 0.00023 | val_loss 1.17570 | train_score 0.99988 | val_score 0.90162 | train_time  16.86 min\n",
      "  2%|â–         | 8/500 [1:28:03<297:42:17, 2178.33s/trial, best loss: -0.9115313463514902]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'p_relu',\n",
       " 'attention': 'Additive',\n",
       " 'attention_dim': 300.0,\n",
       " 'batch_size': 16,\n",
       " 'bias': True,\n",
       " 'freeze': False,\n",
       " 'hidden_dim': 250.0,\n",
       " 'lr_decay': 0.24299504686523796,\n",
       " 'num_layers': 3.0,\n",
       " 'optimizer__lr': 0.0016816180726068677,\n",
       " 'optimizer__wd': 1.4125481325876369e-06,\n",
       " 'p_dropout': 0.45658280816360297,\n",
       " 'softmax': 'LogSoftmax',\n",
       " 'tag_embed_dim': 32,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                            \n",
      "V2_hpo_1.0                                                                                \n",
      "Number of parameters 8728238 trainable 8728238                                            \n",
      "epoch   1/ 20 | train_loss 0.96454 | val_loss 0.99111 | train_score 0.71662 | val_score 0.71293 | train_time   2.00 min *\n",
      "epoch   2/ 20 | train_loss 0.45434 | val_loss 0.53772 | train_score 0.86607 | val_score 0.84423 | train_time   3.99 min *\n",
      "epoch   3/ 20 | train_loss 0.29577 | val_loss 0.47016 | train_score 0.91009 | val_score 0.86610 | train_time   5.98 min *\n",
      "epoch   4/ 20 | train_loss 0.22262 | val_loss 0.46843 | train_score 0.92974 | val_score 0.87342 | train_time   7.97 min *\n",
      "epoch   5/ 20 | train_loss 0.16670 | val_loss 0.47030 | train_score 0.94697 | val_score 0.88008 | train_time   9.97 min *\n",
      "epoch   6/ 20 | train_loss 0.13503 | val_loss 0.49350 | train_score 0.95749 | val_score 0.88296 | train_time  11.96 min *\n",
      "epoch   7/ 20 | train_loss 0.12708 | val_loss 0.53018 | train_score 0.95900 | val_score 0.88004 | train_time  13.95 min\n",
      "epoch   8/ 20 | train_loss 0.09456 | val_loss 0.52334 | train_score 0.96947 | val_score 0.88539 | train_time  15.95 min *\n",
      "epoch   9/ 20 | train_loss 0.07666 | val_loss 0.58280 | train_score 0.97497 | val_score 0.88312 | train_time  17.94 min\n",
      "epoch  10/ 20 | train_loss 0.06879 | val_loss 0.61944 | train_score 0.97733 | val_score 0.88189 | train_time  19.93 min\n",
      "epoch  11/ 20 | train_loss 0.05024 | val_loss 0.61790 | train_score 0.98396 | val_score 0.88641 | train_time  21.93 min *\n",
      "epoch  12/ 20 | train_loss 0.04585 | val_loss 0.66308 | train_score 0.98500 | val_score 0.88621 | train_time  23.92 min\n",
      "epoch  13/ 20 | train_loss 0.04084 | val_loss 0.71313 | train_score 0.98645 | val_score 0.88633 | train_time  25.91 min\n",
      "epoch  14/ 20 | train_loss 0.03695 | val_loss 0.67069 | train_score 0.98790 | val_score 0.88691 | train_time  27.91 min *\n",
      "epoch  15/ 20 | train_loss 0.03298 | val_loss 0.71925 | train_score 0.98908 | val_score 0.88946 | train_time  29.90 min *\n",
      "epoch  16/ 20 | train_loss 0.03167 | val_loss 0.73214 | train_score 0.98924 | val_score 0.88950 | train_time  31.90 min *\n",
      "epoch  17/ 20 | train_loss 0.03162 | val_loss 0.75471 | train_score 0.98948 | val_score 0.88789 | train_time  33.89 min\n",
      "epoch  18/ 20 | train_loss 0.02704 | val_loss 0.79109 | train_score 0.99108 | val_score 0.89015 | train_time  35.88 min *\n",
      "epoch  19/ 20 | train_loss 0.02068 | val_loss 0.76668 | train_score 0.99347 | val_score 0.89110 | train_time  37.87 min *\n",
      "epoch  20/ 20 | train_loss 0.02382 | val_loss 0.78171 | train_score 0.99225 | val_score 0.89102 | train_time  39.86 min\n",
      "epoch  21/ 40 | train_loss 0.02068 | val_loss 0.83003 | train_score 0.99308 | val_score 0.89163 | train_time  41.86 min *\n",
      "epoch  22/ 40 | train_loss 0.01574 | val_loss 0.84853 | train_score 0.99441 | val_score 0.89328 | train_time  43.86 min *\n",
      "epoch  23/ 40 | train_loss 0.00549 | val_loss 0.88359 | train_score 0.99833 | val_score 0.89607 | train_time  45.85 min *\n",
      "epoch  24/ 40 | train_loss 0.00279 | val_loss 0.91028 | train_score 0.99914 | val_score 0.89723 | train_time  47.85 min *\n",
      "epoch  25/ 40 | train_loss 0.00204 | val_loss 0.92027 | train_score 0.99939 | val_score 0.89764 | train_time  49.84 min *\n",
      "epoch  26/ 40 | train_loss 0.00146 | val_loss 0.94820 | train_score 0.99957 | val_score 0.89751 | train_time  51.83 min\n",
      "epoch  27/ 40 | train_loss 0.00129 | val_loss 0.97471 | train_score 0.99962 | val_score 0.89760 | train_time  53.83 min\n",
      "epoch  28/ 40 | train_loss 0.00103 | val_loss 0.98834 | train_score 0.99962 | val_score 0.89760 | train_time  55.82 min\n",
      "epoch  29/ 40 | train_loss 0.00096 | val_loss 1.01316 | train_score 0.99965 | val_score 0.89838 | train_time  57.81 min *\n",
      "epoch  30/ 40 | train_loss 0.00085 | val_loss 1.01283 | train_score 0.99970 | val_score 0.89801 | train_time  59.81 min\n",
      "epoch  31/ 40 | train_loss 0.00081 | val_loss 1.02510 | train_score 0.99970 | val_score 0.89829 | train_time  61.80 min\n",
      "epoch  32/ 40 | train_loss 0.00075 | val_loss 1.02987 | train_score 0.99973 | val_score 0.89792 | train_time  63.80 min\n",
      "epoch  33/ 40 | train_loss 0.00069 | val_loss 1.03634 | train_score 0.99976 | val_score 0.89817 | train_time  65.79 min\n",
      "epoch  34/ 40 | train_loss 0.00068 | val_loss 1.03732 | train_score 0.99978 | val_score 0.89792 | train_time  67.79 min\n",
      "epoch  35/ 40 | train_loss 0.00064 | val_loss 1.03997 | train_score 0.99981 | val_score 0.89755 | train_time  69.79 min\n",
      "epoch  36/ 40 | train_loss 0.00064 | val_loss 1.04470 | train_score 0.99980 | val_score 0.89788 | train_time  71.78 min\n",
      "epoch  37/ 40 | train_loss 0.00062 | val_loss 1.04560 | train_score 0.99981 | val_score 0.89772 | train_time  73.78 min\n",
      "epoch  38/ 40 | train_loss 0.00061 | val_loss 1.04921 | train_score 0.99980 | val_score 0.89764 | train_time  75.77 min\n",
      "epoch  39/ 40 | train_loss 0.00061 | val_loss 1.04926 | train_score 0.99979 | val_score 0.89792 | train_time  77.77 min\n",
      "epoch  40/ 40 | train_loss 0.00060 | val_loss 1.04907 | train_score 0.99979 | val_score 0.89772 | train_time  79.76 min\n",
      "  2%|â–         | 9/500 [2:47:49<403:47:17, 2960.56s/trial, best loss: -0.9115313463514902]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'p_relu',\n",
       " 'attention': 'Additive',\n",
       " 'attention_dim': 300.0,\n",
       " 'batch_size': 16,\n",
       " 'bias': True,\n",
       " 'freeze': False,\n",
       " 'hidden_dim': 250.0,\n",
       " 'lr_decay': 0.1847636931038067,\n",
       " 'num_layers': 4.0,\n",
       " 'optimizer__lr': 0.0017188952953206265,\n",
       " 'optimizer__wd': 4.995869558577694e-06,\n",
       " 'p_dropout': 0.5588284918374005,\n",
       " 'softmax': 'LogSoftmax',\n",
       " 'tag_embed_dim': 32,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                            \n",
      "V2_hpo_1.0                                                                                \n",
      "Number of parameters 10232238 trainable 10232238                                          \n",
      "epoch   1/ 20 | train_loss 1.84529 | val_loss 1.85188 | train_score 0.40756 | val_score 0.40835 | train_time   2.09 min *\n",
      "epoch   2/ 20 | train_loss 0.62279 | val_loss 0.67177 | train_score 0.82357 | val_score 0.81135 | train_time   4.18 min *\n",
      "  2%|â–         | 9/500 [2:52:29<156:50:16, 1149.93s/trial, best loss: -0.9115313463514902]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3d320fd4212b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0mmax_queue_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m              max_evals=iters)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-11a78272cb20>\u001b[0m in \u001b[0;36minit_objective\u001b[0;34m(space, save)\u001b[0m\n\u001b[1;32m     72\u001b[0m                               \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#                       early_stop=5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                               **session)    \n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Neural_Networks_for_Dependency_Parser/src/pytorch_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, device, train_dataset, val_dataset, train_epochs, batch_size, optimizer_params, prints, p_dropout, epochs_save, lr_decay, early_stop, save)\u001b[0m\n\u001b[1;32m    488\u001b[0m                         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_run_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Neural_Networks_for_Dependency_Parser/src/pytorch_utils.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, device, data_loader, train, results, decision_func)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iters = 500\n",
    "\n",
    "_ = hpo.fmin(init_objective,\n",
    "             init_space,\n",
    "             algo=hpo.tpe.suggest,\n",
    "             trials=init_trials,\n",
    "             max_queue_len=1,\n",
    "             max_evals=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "    dill.dump(init_trials, f)\n",
    "init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'activation': 'tanh',\n",
    " 'batch_size': 4.0,\n",
    " 'bias': True,\n",
    " 'hidden_dim': 300.0,\n",
    " 'mlp1_dim': 400.0,\n",
    " 'num_layers': 3.0,\n",
    " 'optimizer__lr': 0.0001461425145455605,\n",
    " 'optimizer__wd': 4.203613249902623e-07,\n",
    " 'p_dropout': 0.19840346306446244,\n",
    " 'tag_embed_dim': 36.0,\n",
    " 'train_epochs': 10,\n",
    " 'word_dropout': 0.29891514439839717,\n",
    " 'word_embed_dim': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ptu.load_model(version=version, versions_dir=versions_dir, epoch='best', seed=42)\n",
    "# loss, score = checkpoint.predict(test_dataset.dataset,\n",
    "#                                  batch_size=32,\n",
    "#                                  device=device,\n",
    "#                                  results=False,\n",
    "#                                  decision_func=chu.test_chu_liu_edmonds)\n",
    "# print(f'chu_liu_edmonds_UAS: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# checkpoint.model = checkpoint.model.to(device)\n",
    "# checkpoint.model.train()\n",
    "# batch_size = 32\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset=train_dataset.dataset, batch_size=batch_size, shuffle=True)\n",
    "# for batch in loader:\n",
    "#     loss, flat_y, flat_out, mask, out, y = utils.loss_decision_func(checkpoint, device, batch, prints=True)\n",
    "#     break\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw2",
   "language": "python",
   "name": "nlp_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
