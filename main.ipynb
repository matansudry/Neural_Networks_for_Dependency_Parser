{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torch.nn.Softmax as softmax\n",
    "\n",
    "import src.pytorch_utils as ptu\n",
    "import src.dataset as dset\n",
    "from src import bilstm\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models_path = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125430/125430 [00:12<00:00, 10131.22it/s]\n",
      "100%|██████████| 25325/25325 [00:02<00:00, 10491.83it/s]\n",
      "100%|██████████| 24744/24744 [00:02<00:00, 11888.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 281 ms, total: 17.2 s\n",
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = dset.DataSet('data/train.labeled', tqdm_bar=True)\n",
    "test_dataset = dset.DataSet('data/test.labeled', tags=train_dataset.tags, words=train_dataset.words, tqdm_bar=True)\n",
    "comp_dataset = dset.DataSet('data/comp.unlabeled', tags=train_dataset.tags, words=train_dataset.words, tqdm_bar=True, tagged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the BiLSTM we found in the internet, need to review the model and compare it to ours \n",
    "vocab_size = len(train_dataset.words)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "version = 0.0\n",
    "\n",
    "checkpoint = ptu.Checkpoint(models_path=models_path,\n",
    "                            version=version,\n",
    "                            model=bilstm.BiLSTM_CRF(vocab_size, tag_to_ix, embedding_dim, hidden_dim),\n",
    "                            score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                            seed=42,\n",
    "                            optimizer=torch.optim.Adam,\n",
    "                            criterion=nn.NLLLoss,\n",
    "                            save=False,\n",
    "                            prints=False,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### matan's code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset class, need to review and keep only the relevant\n",
    "class Dependency_Parser_Dataset(nn.Module):\n",
    "    def __init__(self, word_dict, pos_dict, dir_path: str, subset: str, \n",
    "                 padding=False, word_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.subset = subset # One of the following: [train, test]\n",
    "        self.file = dir_path + subset + \".wtag\"\n",
    "        self.datareader = PosDataReader(self.file, word_dict, pos_dict)\n",
    "        self.vocab_size = len(self.datareader.word_dict)\n",
    "        if word_embeddings:\n",
    "            self.word_idx_mappings, self.idx_word_mappings, self.word_vectors = word_embeddings\n",
    "        else:\n",
    "            self.word_idx_mappings, self.idx_word_mappings, self.word_vectors = ## need to add our word emb hereself.init_word_embeddings(self.datareader.word_dict)\n",
    "        self.pos_idx_mappings, self.idx_pos_mappings = ## need to add our word emb here self.init_pos_vocab(self.datareader.pos_dict)\n",
    "        \n",
    "        self.pad_idx = self.word_idx_mappings.get(PAD_TOKEN)\n",
    "        self.unknown_idx = self.word_idx_mappings.get(UNKNOWN_TOKEN)\n",
    "        self.word_vector_dim = self.word_vectors.size(-1)\n",
    "        self.sentence_lens = [len(sentence) for sentence in self.datareader.sentences]\n",
    "        self.max_seq_len = max(self.sentence_lens)\n",
    "        self.sentences_dataset = self.convert_sentences_to_dataset(padding)\n",
    "    \n",
    "    def get_word_embeddings(self):\n",
    "        return self.word_idx_mappings, self.idx_word_mappings, self.word_vectors\n",
    "\n",
    "    \n",
    "    def init_pos_vocab(self, pos_dict):\n",
    "        idx_pos_mappings = sorted([self.word_idx_mappings.get(token) for token in SPECIAL_TOKENS])\n",
    "        pos_idx_mappings = {self.idx_word_mappings[idx]: idx for idx in idx_pos_mappings}\n",
    "        \n",
    "        \n",
    "    def convert_sentences_to_dataset(self, padding):\n",
    "        sentence_word_idx_list = list()\n",
    "        sentence_pos_idx_list = list()\n",
    "        sentence_len_list = list()\n",
    "        for sentence_idx, sentence in enumerate(self.datareader.sentences):\n",
    "            words_idx_list = []\n",
    "            pos_idx_list = []\n",
    "            for word, pos in sentence:\n",
    "                words_idx_list.append(self.word_idx_mappings.get(word))\n",
    "                pos_idx_list.append(self.pos_idx_mappings.get(pos))\n",
    "            sentence_len = len(words_idx_list)\n",
    "            # if padding:\n",
    "            #     while len(words_idx_list) < self.max_seq_len:\n",
    "            #         words_idx_list.append(self.word_idx_mappings.get(PAD_TOKEN))\n",
    "            #         pos_idx_list.append(self.pos_idx_mappings.get(PAD_TOKEN))\n",
    "            sentence_word_idx_list.append(torch.tensor(words_idx_list, dtype=torch.long, requires_grad=False))\n",
    "            sentence_pos_idx_list.append(torch.tensor(pos_idx_list, dtype=torch.long, requires_grad=False))\n",
    "            sentence_len_list.append(sentence_len)\n",
    "        \n",
    "        # if padding:\n",
    "        #     all_sentence_word_idx = torch.tensor(sentence_word_idx_list, dtype=torch.long)\n",
    "        #     all_sentence_pos_idx = torch.tensor(sentence_pos_idx_list, dtype=torch.long)\n",
    "        #     all_sentence_len = torch.tensor(sentence_len_list, dtype=torch.long, requires_grad=False)\n",
    "        #     return TensorDataset(all_sentence_word_idx, all_sentence_pos_idx, all_sentence_len)\n",
    "            \n",
    "        return {i: sample_tuple for i, sample_tuple in enumerate(zip(sentence_word_idx_list,\n",
    "                                                                     sentence_pos_idx_list,\n",
    "                                                                     sentence_len_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## our model\n",
    "class Dnn_Dependency_Parser(nn.Module):\n",
    "    def __init__(self, word_embeddings, hidden_dim, word_vocab_size, tag_vocab_size):\n",
    "        super(Dnn_Dependency_Parser, self).__init__()\n",
    "        emb_dim = word_embeddings.shape[1]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.word_embedding = ## need to add our embeddin - nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        # self.word_embedding = nn.Embedding.from_pretrained(word_embeddings, freeze=False)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, num_layers=2, bidirectional=True, batch_first=False)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tag_vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, word_idx_tensor):\n",
    "        embeds = self.word_embedding(word_idx_tensor.to(self.device))   # [batch_size, seq_length, emb_dim]      \n",
    "        lstm_out, _ = self.lstm(embeds.view(embeds.shape[1], 1, -1))    # [seq_length, batch_size, 2*hidden_dim]\n",
    "        tag_space = self.hidden2tag(lstm_out.view(embeds.shape[1], -1)) # [seq_length, tag_dim]\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)                    # [seq_length, tag_dim]\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1  \n",
    "\n",
    "EPOCHS = 15\n",
    "WORD_EMBEDDING_DIM = 100 ## need to decide if this is the right DIM\n",
    "HIDDEN_DIM = 1000 ## need to decide if this is the right DIM\n",
    "word_vocab_size = len(train.word_idx_mappings)\n",
    "tag_vocab_size = len(train.pos_idx_mappings)\n",
    "\n",
    "##need to decide whihc parameters are relevant\n",
    "model = Dnn_Dependency_Parser(train_dataloader.dataset.word_vectors, HIDDEN_DIM, word_vocab_size, tag_vocab_size)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model.cuda()\n",
    "\n",
    "# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\n",
    "\n",
    "## no need of that, i implement Negative_log_Likelihood_Loss\n",
    "#loss_function = nn.NLLLoss()\n",
    "\n",
    "# We will be using a simple SGD optimizer to minimize the loss function\n",
    "## we are ok with Adam? need to change?\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "acumulate_grad_steps = 50 # This is the actual batch_size, while we officially use batch_size=1\n",
    "\n",
    "# Training start\n",
    "print(\"Training Started\")\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "epochs = EPOCHS\n",
    "for epoch in range(epochs):\n",
    "    acc = 0 # to keep track of accuracy\n",
    "    printable_loss = 0 # To keep track of the loss value\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):\n",
    "        i += 1\n",
    "        words_idx_tensor, pos_idx_tensor, sentence_length = input_data\n",
    "        \n",
    "        sentence_scores = model(words_idx_tensor)\n",
    "        sentence_scores = ## need to fix it ----tag_scores.unsqueeze(0).permute(0,2,1)\n",
    "        #print(\"tag_scores shape -\", tag_scores.shape)\n",
    "        #print(\"pos_idx_tensor shape -\", pos_idx_tensor.shape)\n",
    "        loss = Negative_log_Likelihood_Loss(dataset, network_parameters) # need to fix network_parameters\n",
    "        loss = loss / acumulate_grad_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        printable_loss += loss.item()## we need to change loss.item() to our lass, i think it will be only loss\n",
    "        _, indices = torch.max(sentence_scores, 1)\n",
    "        # print(\"tag_scores shape-\", tag_scores.shape)\n",
    "        # print(\"indices shape-\", indices.shape)\n",
    "        # acc += indices.eq(pos_idx_tensor.view_as(indices)).mean().item()\n",
    "        acc += torch.mean(torch.tensor(pos_idx_tensor.to(\"cpu\") == indices.to(\"cpu\"), dtype=torch.float))##i think we should fix it\n",
    "    printable_loss = printable_loss / len(train)\n",
    "    acc = acc / len(train)\n",
    "    loss_list.append(float(printable_loss))\n",
    "    accuracy_list.append(float(acc))\n",
    "    test_acc = evaluate()\n",
    "    e_interval = i\n",
    "    print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\\t Test Accuracy: {}\".format(epoch + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:]), test_acc))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function\n",
    "def Negative_log_Likelihood_Loss(dataset, network_parameters):\n",
    "    loss = 0\n",
    "    for x_i, y_i in dataset:\n",
    "        softmax_score = softmax(y_i)\n",
    "        for head, modifier in y_i:\n",
    "            loss -=(1/absoulte_y_i(y_i))*mat.log(softmax_score(head,modifer))\n",
    "              \n",
    "def absoulte_y_i(y_i):\n",
    "    return len(y_i[:0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate functino- i used the function they share with us,looks ok for me \n",
    "def evaluate(test_dataloader):\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, input_data in enumerate(test_dataloader):\n",
    "            \n",
    "            words_idx_tensor, pos_idx_tensor, sentence_length = input_data  \n",
    "            tag_scores = model(words_idx_tensor)\n",
    "            tag_scores = tag_scores.unsqueeze(0).permute(0,2,1)\n",
    "            \n",
    "            _, indices = torch.max(tag_scores, 1)\n",
    "            acc += torch.mean(torch.tensor(pos_idx_tensor.to(\"cpu\") == indices.to(\"cpu\"), dtype=torch.float))\n",
    "        acc = acc / len(test)\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw2",
   "language": "python",
   "name": "nlp_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
