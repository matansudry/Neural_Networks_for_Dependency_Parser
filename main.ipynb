{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import src.pytorch_utils as ptu\n",
    "import src.dataset as dset\n",
    "from src import bilstm\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models_path = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125430/125430 [00:13<00:00, 8964.33it/s]\n",
      "100%|██████████| 25325/25325 [00:02<00:00, 8795.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dset.DataSet('data/train.labeled', tqdm_bar=True, pad=False)\n",
    "# test_dataset = dset.DataSet('data/test.labeled', tqdm_bar=True)\n",
    "test_dataset = dset.DataSet('data/test.labeled', train_dataset=train_dataset, tqdm_bar=True, pad=False)\n",
    "# comp_dataset = dset.DataSet('data/comp.unlabeled', train_dataset=train_dataset, tagged=False, tqdm_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_torch = torch.utils.data.TensorDataset(train_dataset.X, train_dataset.y)\n",
    "# test_dataset_torch = torch.utils.data.TensorDataset(test_dataset.X, test_dataset.y)\n",
    "\n",
    "train_dataset_torch = list(zip(train_dataset.X, train_dataset.y))\n",
    "test_dataset_torch = list(zip(test_dataset.X, test_dataset.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, train_dataset, word_embed_dim, tag_embed_dim, hidden_dim, num_layers, bias, p_dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.word_embedding_layer = nn.Embedding(num_embeddings=len(train_dataset.words),\n",
    "                                                 embedding_dim=word_embed_dim,\n",
    "                                                 padding_idx=int(train_dataset.special[dset.PAD]))\n",
    "\n",
    "        self.tag_embedding_layer = nn.Embedding(num_embeddings=len(train_dataset.tags),\n",
    "                                                embedding_dim=tag_embed_dim,\n",
    "                                                padding_idx=int(train_dataset.special[dset.PAD]))\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=word_embed_dim + tag_embed_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=bias,\n",
    "                            dropout=p_dropout,\n",
    "                            batch_first=False,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Linear(int(hidden_dim*2),\n",
    "                                len(train_dataset.token_heads),\n",
    "                                bias=bias)\n",
    "\n",
    "    def forward(self, x, prints=False):\n",
    "        if prints:\n",
    "            print('input', x.shape)\n",
    "        word_embeds = self.word_embedding_layer(x[:, :, 0]) # [batch_size, max_sentence_len, word_embed_dim]\n",
    "        if prints:\n",
    "            print('word_embeds', word_embeds.shape)\n",
    "        tag_embeds = self.tag_embedding_layer(x[:, :, 1])   # [batch_size, max_sentence_len, tag_embed_dim]\n",
    "        if prints:\n",
    "            print('tag_embeds', tag_embeds.shape)\n",
    "        x = torch.cat((word_embeds, tag_embeds), -1)         # []\n",
    "        if prints:\n",
    "            print('cat', x.shape)\n",
    "        x, _ = self.lstm(x.view(x.shape[1], x.shape[0], -1))    # [seq_length, batch_size, 2*hidden_dim]\n",
    "        if prints:\n",
    "            print('lstm', x.shape)\n",
    "        x = self.linear(x.transpose(1, 0))\n",
    "        if prints:\n",
    "            print('linear', x.shape)\n",
    "        x = F.log_softmax(x, dim=2)\n",
    "        if prints:\n",
    "            print('log_softmax', x.shape)\n",
    "        x = x.transpose(2, 1)\n",
    "        if prints:\n",
    "            print('transpose', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 0.0\n",
    "\n",
    "model = BiLSTM(train_dataset, word_embed_dim=300, tag_embed_dim=300, hidden_dim=512, num_layers=1, bias=True, p_dropout=0.0)\n",
    "\n",
    "checkpoint = ptu.Checkpoint(models_path=models_path,\n",
    "                            version=version,\n",
    "                            model=model,\n",
    "                            score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                            decision_func=lambda x: x.argmax(axis=1),#.view(x.shape[0], -1),\n",
    "                            seed=42,\n",
    "                            optimizer=torch.optim.Adam,\n",
    "                            criterion=nn.NLLLoss,\n",
    "                            save=False,\n",
    "                            prints=False,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1fbc84eb4f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                      \u001b[0mepochs_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                      \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                      **session)\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/Neural_Networks_for_Dependency_Parser/src/pytorch_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, device, train_dataset, val_dataset, train_epochs, batch_size, optimizer_params, prints, p_dropout, epochs_save, lr_decay, save)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;31m# train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/alexz/OneDrive/MainEnv/nlp/Neural_Networks_for_Dependency_Parser/src/pytorch_utils.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, device, data_loader, train)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ca84af8a493e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prints)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# [seq_length, batch_size, 2*hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/nlp_hw2/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 570\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparam_list = [\n",
    "    {'train_epochs': 50, 'batch_size': 32, 'optimizer_params': {'lr': 4e-4, 'weight_decay': 0}, 'lr_decay': 0.0},\n",
    "#     {'train_epochs': 10, 'batch_size': 32, 'optimizer_params': {'lr': 1e-4, 'weight_decay': 0}, 'p_dropout': 0.5, 'lr_decay': 0.07},\n",
    "#     {'train_epochs': 10, 'batch_size': 64, 'optimizer_params': {'lr': 1e-5, 'weight_decay': 0}, 'p_dropout': 0.5, 'lr_decay': 0.07},\n",
    "#     {'train_epochs': 10, 'batch_size': 128, 'optimizer_params': {'lr': 1e-6, 'weight_decay': 0}, 'p_dropout': 0.5, 'lr_decay': 0.07},\n",
    "]\n",
    "\n",
    "for session in hyperparam_list:\n",
    "    checkpoint.train(device=device,\n",
    "                     train_dataset=train_dataset_torch,\n",
    "                     val_dataset=test_dataset_torch,\n",
    "                     prints=True,\n",
    "                     epochs_save=1,\n",
    "                     save=False,\n",
    "                     **session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "loss_sum = np.array([])\n",
    "y_pred = np.array([])\n",
    "y_true = np.array([])\n",
    "\n",
    "counter = 0\n",
    "for batch in torch.utils.data.DataLoader(dataset=test_dataset_torch, batch_size=1, shuffle=True):\n",
    "    for i, _ in enumerate(batch):\n",
    "        batch[i] = batch[i].to(device)\n",
    "    out = checkpoint.model.forward(*batch[:-1], prints=True)\n",
    "    print('out', out.shape)\n",
    "    print('true', batch[-1].long().shape)\n",
    "    loss = checkpoint.criterion(out, batch[-1].long())\n",
    "\n",
    "    loss_sum = np.append(loss_sum, float(loss.data))\n",
    "    \n",
    "    y_pred = np.append(y_pred, checkpoint.decision_func(out.detach().cpu().numpy()))\n",
    "    \n",
    "    y_true = np.append(y_true, batch[-1].detach().cpu().numpy())\n",
    "    counter += 1\n",
    "    if counter > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### matan's code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset class, need to review and keep only the relevant\n",
    "class Dependency_Parser_Dataset(nn.Module):\n",
    "    def __init__(self, word_dict, pos_dict, dir_path: str, subset: str, \n",
    "                 padding=False, word_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.subset = subset # One of the following: [train, test]\n",
    "        self.file = dir_path + subset + \".wtag\"\n",
    "        self.datareader = PosDataReader(self.file, word_dict, pos_dict)\n",
    "        self.vocab_size = len(self.datareader.word_dict)\n",
    "        if word_embeddings:\n",
    "            self.word_idx_mappings, self.idx_word_mappings, self.word_vectors = word_embeddings\n",
    "        else:\n",
    "            self.word_idx_mappings, self.idx_word_mappings, self.word_vectors = ## need to add our word emb hereself.init_word_embeddings(self.datareader.word_dict)\n",
    "        self.pos_idx_mappings, self.idx_pos_mappings = ## need to add our word emb here self.init_pos_vocab(self.datareader.pos_dict)\n",
    "        \n",
    "        self.pad_idx = self.word_idx_mappings.get(PAD_TOKEN)\n",
    "        self.unknown_idx = self.word_idx_mappings.get(UNKNOWN_TOKEN)\n",
    "        self.word_vector_dim = self.word_vectors.size(-1)\n",
    "        self.sentence_lens = [len(sentence) for sentence in self.datareader.sentences]\n",
    "        self.max_seq_len = max(self.sentence_lens)\n",
    "        self.sentences_dataset = self.convert_sentences_to_dataset(padding)\n",
    "    \n",
    "    def get_word_embeddings(self):\n",
    "        return self.word_idx_mappings, self.idx_word_mappings, self.word_vectors\n",
    "\n",
    "    \n",
    "    def init_pos_vocab(self, pos_dict):\n",
    "        idx_pos_mappings = sorted([self.word_idx_mappings.get(token) for token in SPECIAL_TOKENS])\n",
    "        pos_idx_mappings = {self.idx_word_mappings[idx]: idx for idx in idx_pos_mappings}\n",
    "        \n",
    "        \n",
    "    def convert_sentences_to_dataset(self, padding):\n",
    "        sentence_word_idx_list = list()\n",
    "        sentence_pos_idx_list = list()\n",
    "        sentence_len_list = list()\n",
    "        for sentence_idx, sentence in enumerate(self.datareader.sentences):\n",
    "            words_idx_list = []\n",
    "            pos_idx_list = []\n",
    "            for word, pos in sentence:\n",
    "                words_idx_list.append(self.word_idx_mappings.get(word))\n",
    "                pos_idx_list.append(self.pos_idx_mappings.get(pos))\n",
    "            sentence_len = len(words_idx_list)\n",
    "            # if padding:\n",
    "            #     while len(words_idx_list) < self.max_seq_len:\n",
    "            #         words_idx_list.append(self.word_idx_mappings.get(PAD_TOKEN))\n",
    "            #         pos_idx_list.append(self.pos_idx_mappings.get(PAD_TOKEN))\n",
    "            sentence_word_idx_list.append(torch.tensor(words_idx_list, dtype=torch.long, requires_grad=False))\n",
    "            sentence_pos_idx_list.append(torch.tensor(pos_idx_list, dtype=torch.long, requires_grad=False))\n",
    "            sentence_len_list.append(sentence_len)\n",
    "        \n",
    "        # if padding:\n",
    "        #     all_sentence_word_idx = torch.tensor(sentence_word_idx_list, dtype=torch.long)\n",
    "        #     all_sentence_pos_idx = torch.tensor(sentence_pos_idx_list, dtype=torch.long)\n",
    "        #     all_sentence_len = torch.tensor(sentence_len_list, dtype=torch.long, requires_grad=False)\n",
    "        #     return TensorDataset(all_sentence_word_idx, all_sentence_pos_idx, all_sentence_len)\n",
    "            \n",
    "        return {i: sample_tuple for i, sample_tuple in enumerate(zip(sentence_word_idx_list,\n",
    "                                                                     sentence_pos_idx_list,\n",
    "                                                                     sentence_len_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## our model\n",
    "class Dnn_Dependency_Parser(nn.Module):\n",
    "    def __init__(self, word_embeddings, hidden_dim, word_vocab_size, tag_vocab_size):\n",
    "        super(Dnn_Dependency_Parser, self).__init__()\n",
    "        emb_dim = word_embeddings.shape[1]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.word_embedding = ## need to add our embeddin - nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        # self.word_embedding = nn.Embedding.from_pretrained(word_embeddings, freeze=False)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, num_layers=2, bidirectional=True, batch_first=False)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tag_vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, word_idx_tensor):\n",
    "        embeds = self.word_embedding(word_idx_tensor.to(self.device))   # [batch_size, seq_length, emb_dim]      \n",
    "        lstm_out, _ = self.lstm(embeds.view(embeds.shape[1], 1, -1))    # [seq_length, batch_size, 2*hidden_dim]\n",
    "        tag_space = self.hidden2tag(lstm_out.view(embeds.shape[1], -1)) # [seq_length, tag_dim]\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)                    # [seq_length, tag_dim]\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1  \n",
    "\n",
    "EPOCHS = 15\n",
    "WORD_EMBEDDING_DIM = 100 ## need to decide if this is the right DIM\n",
    "HIDDEN_DIM = 1000 ## need to decide if this is the right DIM\n",
    "word_vocab_size = len(train.word_idx_mappings)\n",
    "tag_vocab_size = len(train.pos_idx_mappings)\n",
    "\n",
    "##need to decide whihc parameters are relevant\n",
    "model = Dnn_Dependency_Parser(train_dataloader.dataset.word_vectors, HIDDEN_DIM, word_vocab_size, tag_vocab_size)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model.cuda()\n",
    "\n",
    "# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\n",
    "\n",
    "## no need of that, i implement Negative_log_Likelihood_Loss\n",
    "#loss_function = nn.NLLLoss()\n",
    "\n",
    "# We will be using a simple SGD optimizer to minimize the loss function\n",
    "## we are ok with Adam? need to change?\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "acumulate_grad_steps = 50 # This is the actual batch_size, while we officially use batch_size=1\n",
    "\n",
    "# Training start\n",
    "print(\"Training Started\")\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "epochs = EPOCHS\n",
    "for epoch in range(epochs):\n",
    "    acc = 0 # to keep track of accuracy\n",
    "    printable_loss = 0 # To keep track of the loss value\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):\n",
    "        i += 1\n",
    "        words_idx_tensor, pos_idx_tensor, sentence_length = input_data\n",
    "        \n",
    "        sentence_scores = model(words_idx_tensor)\n",
    "        sentence_scores = ## need to fix it ----tag_scores.unsqueeze(0).permute(0,2,1)\n",
    "        #print(\"tag_scores shape -\", tag_scores.shape)\n",
    "        #print(\"pos_idx_tensor shape -\", pos_idx_tensor.shape)\n",
    "        loss = Negative_log_Likelihood_Loss(dataset, network_parameters) # need to fix network_parameters\n",
    "        loss = loss / acumulate_grad_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        printable_loss += loss.item()## we need to change loss.item() to our lass, i think it will be only loss\n",
    "        _, indices = torch.max(sentence_scores, 1)\n",
    "        # print(\"tag_scores shape-\", tag_scores.shape)\n",
    "        # print(\"indices shape-\", indices.shape)\n",
    "        # acc += indices.eq(pos_idx_tensor.view_as(indices)).mean().item()\n",
    "        acc += torch.mean(torch.tensor(pos_idx_tensor.to(\"cpu\") == indices.to(\"cpu\"), dtype=torch.float))##i think we should fix it\n",
    "    printable_loss = printable_loss / len(train)\n",
    "    acc = acc / len(train)\n",
    "    loss_list.append(float(printable_loss))\n",
    "    accuracy_list.append(float(acc))\n",
    "    test_acc = evaluate()\n",
    "    e_interval = i\n",
    "    print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\\t Test Accuracy: {}\".format(epoch + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:]), test_acc))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function\n",
    "def Negative_log_Likelihood_Loss(dataset, network_parameters):\n",
    "    loss = 0\n",
    "    for x_i, y_i in dataset:\n",
    "        softmax_score = softmax(y_i)\n",
    "        for head, modifier in y_i:\n",
    "            loss -=(1/absoulte_y_i(y_i))*mat.log(softmax_score(head,modifer))\n",
    "              \n",
    "def absoulte_y_i(y_i):\n",
    "    return len(y_i[:0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate functino- i used the function they share with us,looks ok for me \n",
    "def evaluate(test_dataloader):\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, input_data in enumerate(test_dataloader):\n",
    "            \n",
    "            words_idx_tensor, pos_idx_tensor, sentence_length = input_data  \n",
    "            tag_scores = model(words_idx_tensor)\n",
    "            tag_scores = tag_scores.unsqueeze(0).permute(0,2,1)\n",
    "            \n",
    "            _, indices = torch.max(tag_scores, 1)\n",
    "            acc += torch.mean(torch.tensor(pos_idx_tensor.to(\"cpu\") == indices.to(\"cpu\"), dtype=torch.float))\n",
    "        acc = acc / len(test)\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw2",
   "language": "python",
   "name": "nlp_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
