{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "# import hyperopt as hpo\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src import utils\n",
    "from src import model2 as m2\n",
    "import src.dataset as dset\n",
    "import src.pytorch_utils as ptu\n",
    "import src.chu_liu_edmonds as chu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "versions_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dset.DataSet('data/train.labeled', tqdm_bar=True, use_glove=True)\n",
    "# test_dataset = dset.DataSet('data/test.labeled', train_dataset=train_dataset, tqdm_bar=True, use_glove=True)\n",
    "# comp_dataset = dset.DataSet('data/comp.unlabeled', train_dataset=train_dataset, tagged=False, tqdm_bar=True, use_glove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join('data', 'train_dataset.pth'), 'wb') as f:\n",
    "#     dill.dump(train_dataset, f)\n",
    "# with open(os.path.join('data', 'test_dataset.pth'), 'wb') as f:\n",
    "#     dill.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train_dataset.pth'), \"rb\") as f:\n",
    "    train_dataset = dill.load(f)\n",
    "with open(os.path.join('data', 'test_dataset.pth'), \"rb\") as f:\n",
    "    test_dataset = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = dict(sorted(list({\n",
    "    'tanh': nn.Tanh(),\n",
    "    'hard_tanh': nn.Hardtanh(),\n",
    "#     'relu': nn.ReLU(),\n",
    "#     'elu': nn.ELU(),\n",
    "#     'leaky_relu': nn.LeakyReLU(),\n",
    "#     'p_relu': nn.PReLU(),\n",
    "#     'relu6': nn.ReLU6(),\n",
    "#     'gelu': nn.GELU(),\n",
    "#     'sigmoid': nn.Sigmoid(),\n",
    "}.items()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'V2_1.5'\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ptu.load_model(version=version, versions_dir=versions_dir, epoch=-1, seed=42)\n",
    "# display(checkpoint.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: V2_1.5\n",
      "Number of parameters 5882137 trainable 5882137\n"
     ]
    }
   ],
   "source": [
    "model = m2.Model2(train_dataset=train_dataset,\n",
    "                  word_embed_dim=300,  # 300\n",
    "                  tag_embed_dim=32,  # 32\n",
    "                  hidden_dim=250,  # 125\n",
    "                  num_layers=4,  # 2\n",
    "                  bias=True,  # True\n",
    "                  attention_dim=200,  # 10\n",
    "                  activation=activations['tanh'],\n",
    "                  p_dropout=0.45,  # 0.5\n",
    "                  word_dropout=0.25,  # 0.0\n",
    "                  attention=utils.AdditiveAttention,\n",
    "#                   attention=utils.DotAttention,\n",
    "#                   attention=utils.MultiplicativeAttention,\n",
    "                  softmax=nn.LogSoftmax(dim=2),\n",
    "#                   softmax=nn.Softmax(dim=2),\n",
    "                  glove=True,\n",
    "                  positional_encoding=False)\n",
    "\n",
    "checkpoint = ptu.Checkpoint(versions_dir=versions_dir,\n",
    "                            version=version,\n",
    "                            model=model,\n",
    "                            score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                            loss_decision_func=utils.loss_decision_func,\n",
    "                            out_decision_func=lambda y_pred, flat_y_pred, mask, padding: flat_y_pred.argmax(axis=1),\n",
    "                            seed=42,\n",
    "                            optimizer=torch.optim.Adam,\n",
    "                            criterion=nn.NLLLoss,\n",
    "                            save=save,\n",
    "                            prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/  5 | train_loss 1.85642 | val_loss 1.83721 | train_score 0.44313 | val_score 0.44625 | train_time   1.29 min *\n",
      "epoch   2/  5 | train_loss 0.89174 | val_loss 0.87620 | train_score 0.75149 | val_score 0.76140 | train_time   3.41 min *\n",
      "epoch   3/  5 | train_loss 0.65940 | val_loss 0.66200 | train_score 0.80939 | val_score 0.81085 | train_time   5.61 min *\n"
     ]
    }
   ],
   "source": [
    "wd = 1e-6\n",
    "lr = 1.5e-3\n",
    "lr_deacy = 0.2\n",
    "betas = (0.95, 0.998)\n",
    "\n",
    "hyperparam_list = [\n",
    "    {'train_epochs':  5, 'optimizer_params': {'lr': lr, 'weight_decay': wd, 'betas': betas}},\n",
    "    {'train_epochs': 15, 'optimizer_params': {'lr': lr, 'weight_decay': wd, 'betas': betas}, 'lr_decay': lr_deacy},\n",
    "    {'train_epochs':  5, 'optimizer_params': {'weight_decay': wd, 'betas': betas}},\n",
    "    {'train_epochs': 15, 'optimizer_params': {'lr': lr, 'weight_decay': wd, 'betas': betas}, 'lr_decay': lr_deacy},\n",
    "    {'train_epochs':  5, 'optimizer_params': {'weight_decay': wd, 'betas': betas}},\n",
    "    {'train_epochs': 15, 'optimizer_params': {'lr': lr, 'weight_decay': wd, 'betas': betas}, 'lr_decay': lr_deacy},\n",
    "    {'train_epochs':  5, 'optimizer_params': {'weight_decay': wd, 'betas': betas}},\n",
    "]\n",
    "\n",
    "for session in hyperparam_list:\n",
    "    checkpoint.train(device=device,\n",
    "                     train_dataset=train_dataset.dataset,\n",
    "                     val_dataset=test_dataset.dataset,\n",
    "                     prints=True,\n",
    "                     epochs_save=5,\n",
    "                     save=save,\n",
    "#                      early_stop=5,\n",
    "                     batch_size=16,\n",
    "                     **session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'V2_hpo_1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_space = dict(sorted(list({\n",
    "    'train_epochs': 50,\n",
    "    'batch_size': hpo.hp.quniform('batch_size', low=4, high=5, q=1),  # 16-32-64\n",
    "    'optimizer__lr': hpo.hp.uniform('optimizer__lr', low=1e-4, high=1e-3),\n",
    "    'optimizer__wd': hpo.hp.choice('optimizer__wd_ind', [0, hpo.hp.uniform('optimizer__wd', low=0, high=1e-6)]),# 0.0\n",
    "    'bias': True, #hpo.hp.choice('bias', [True, False]),\n",
    "    'early_stop': 5,\n",
    "    \n",
    "    'word_embed_dim': 300,  # 300\n",
    "    'tag_embed_dim': 32, #hpo.hp.quniform('tag_embed_dim', low=30, high=50, q=4), #25\n",
    "    'hidden_dim': hpo.hp.quniform('hidden_dim', low=100, high=300, q=50), #125,  # \n",
    "    'num_layers': hpo.hp.quniform('num_layers', low=2, high=4, q=1),#2,  # \n",
    "    'mlp1_dim': hpo.hp.quniform('mlp1_dim', low=100, high=300, q=50),#100,  # \n",
    "    'activation': hpo.hp.choice('activation', list(activations.keys())),#nn.Tanh(),  # \n",
    "    'p_dropout': hpo.hp.normal('p_dropout', mu=0.1, sigma=0.1),#0.1,  # \n",
    "    'word_dropout': hpo.hp.normal('word_dropout', mu=0.1, sigma=0.1),#0.25,  # \n",
    "}.items()), key=lambda x: x[0]))\n",
    "\n",
    "def init_objective(space, save=False):\n",
    "    display(space)\n",
    "    last_score = init_log['test_score'].max() if len(init_log) > 0 else 0.0\n",
    "    batch_size = min(len(train_dataset.dataset), int(2 ** space['batch_size']))\n",
    "    p_dropout = max(0.0, min(0.7, space['p_dropout']))\n",
    "    word_dropout = max(0.0, min(0.7, space['word_dropout']))\n",
    "\n",
    "    model = m2.Model2(train_dataset=train_dataset,\n",
    "                      word_embed_dim=300,\n",
    "                      tag_embed_dim=int(space['tag_embed_dim']),\n",
    "                      hidden_dim=int(space['hidden_dim']),\n",
    "                      num_layers=int(space['num_layers']),\n",
    "                      bias=space['bias'],\n",
    "                      mlp1_dim=int(space['mlp1_dim']),\n",
    "                      activation=activations[space['activation']],\n",
    "                      p_dropout=p_dropout,\n",
    "                      word_dropout=word_dropout,\n",
    "                      glove=True,\n",
    "                      positional_encoding=False)\n",
    "\n",
    "    init_checkpoint = ptu.Checkpoint(version=version,\n",
    "                                     model=model,\n",
    "                                     optimizer=torch.optim.Adam,\n",
    "                                     criterion=nn.NLLLoss,\n",
    "                                     score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                                     versions_dir=versions_dir,\n",
    "                                     loss_decision_func=utils.loss_decision_func,\n",
    "                                     out_decision_func=lambda y_pred, flat_y_pred, mask, padding: flat_y_pred.argmax(axis=1),\n",
    "                                     seed=42,\n",
    "                                     custom_run_func=None,\n",
    "                                     save=False,\n",
    "                                     prints=True)\n",
    "    \n",
    "    init_checkpoint.train(device=device,\n",
    "                          train_dataset=train_dataset.dataset,\n",
    "                          val_dataset=test_dataset.dataset,\n",
    "                          train_epochs=space['train_epochs'],\n",
    "                          batch_size=batch_size,\n",
    "                          optimizer_params={\n",
    "                              'lr': space['optimizer__lr'],\n",
    "                              'weight_decay': space['optimizer__wd'],\n",
    "                          },\n",
    "                          prints=True,\n",
    "                          epochs_save=0,\n",
    "                          early_stop=space['early_stop'],\n",
    "                          save=save)\n",
    "    \n",
    "    train_score = init_checkpoint.get_log(col='train_score', epoch=-1)\n",
    "    test_score = init_checkpoint.get_log(col='val_score', epoch=-1)\n",
    "#     print('test_score', test_score)\n",
    "    ###############################################################\n",
    "    if test_score > last_score:\n",
    "        init_checkpoint.save(epoch=True)\n",
    "    init_log.loc[init_log.index.max() + 1 if len(init_log) > 0 else 0] = [time.strftime('%d-%m-%Y %H:%M:%S'),\n",
    "#                                                                           train_score,\n",
    "                                                                          test_score,\n",
    "                                                                          space] + list(space.values())\n",
    "    \n",
    "    with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "        dill.dump(init_trials, f)\n",
    "    init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)\n",
    "\n",
    "    return -test_score\n",
    "\n",
    "# session_space = dict(sorted(list({\n",
    "#     'train_epochs': 5,\n",
    "#     'batch_size_mult': min(len(X_train), int(2**hpo.hp.quniform('batch_size_mult', low=5, high=9, q=1))),\n",
    "#     'optimizer__lr_mult': hpo.hp.uniform('optimizer__lr_mult', low=1e-5, high=1e-3),\n",
    "#     'optimizer__weight_decay': hpo.hp.uniform('optimizer__weight_decay', low=1e-5, high=1e-3),\n",
    "#     'p_dropout': max(0.0, min(0.9, hpo.hp.normal('p_dropout', mu=0.5, sigma=0.15))),\n",
    "# }.items()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_trials = hpo.Trials()\n",
    "# init_log = pd.DataFrame(columns=['timestamp',\n",
    "#                                  # 'train_score',\n",
    "#                                  'test_score',\n",
    "#                                  'space'] + list(init_space.keys()))\n",
    "\n",
    "# with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "#     dill.dump(init_trials, f)\n",
    "# init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(versions_dir, version, 'trials.pth'), \"rb\") as f:\n",
    "    init_trials = dill.load(f)\n",
    "init_log = pd.read_csv(os.path.join(versions_dir, version, 'trials_log.csv'))\n",
    "# display(init_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/500 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'gelu',\n",
       " 'batch_size': 4.0,\n",
       " 'bias': True,\n",
       " 'early_stop': 5,\n",
       " 'hidden_dim': 150.0,\n",
       " 'mlp1_dim': 150.0,\n",
       " 'num_layers': 4.0,\n",
       " 'optimizer__lr': 0.0007100446481541015,\n",
       " 'optimizer__wd': 0,\n",
       " 'p_dropout': -0.017004121103458253,\n",
       " 'tag_embed_dim': 32,\n",
       " 'train_epochs': 50,\n",
       " 'word_dropout': 0.14995879743817125,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                         \n",
      "V2_hpo_1.0                                                      \n",
      "Number of parameters 2299987 trainable 2299987                  \n",
      "epoch   1/ 50 | train_loss 1.66794 | val_loss 1.65149 | train_score 0.45373 | val_score 0.45751 | train_time   0.92 min *\n",
      "epoch   2/ 50 | train_loss 0.86116 | val_loss 0.88207 | train_score 0.75105 | val_score 0.74730 | train_time   1.83 min *\n",
      "epoch   3/ 50 | train_loss 0.64424 | val_loss 0.69090 | train_score 0.81823 | val_score 0.80711 | train_time   2.75 min *\n",
      "epoch   4/ 50 | train_loss 0.52722 | val_loss 0.60004 | train_score 0.84948 | val_score 0.83215 | train_time   3.67 min *\n",
      "epoch   5/ 50 | train_loss 0.46754 | val_loss 0.56679 | train_score 0.86385 | val_score 0.83877 | train_time   4.58 min *\n",
      "epoch   6/ 50 | train_loss 0.40919 | val_loss 0.53510 | train_score 0.87442 | val_score 0.84711 | train_time   5.50 min *\n",
      "epoch   7/ 50 | train_loss 0.37172 | val_loss 0.52705 | train_score 0.88363 | val_score 0.85040 | train_time   6.42 min *\n",
      "epoch   8/ 50 | train_loss 0.35836 | val_loss 0.55344 | train_score 0.88703 | val_score 0.84678 | train_time   7.33 min\n",
      "epoch   9/ 50 | train_loss 0.33134 | val_loss 0.56897 | train_score 0.89595 | val_score 0.84843 | train_time   8.25 min\n",
      "epoch  10/ 50 | train_loss 0.29780 | val_loss 0.57179 | train_score 0.90814 | val_score 0.85636 | train_time   9.17 min *\n",
      "epoch  11/ 50 | train_loss 0.28573 | val_loss 0.57956 | train_score 0.91187 | val_score 0.85801 | train_time  10.08 min *\n",
      "epoch  12/ 50 | train_loss 0.28509 | val_loss 0.60359 | train_score 0.91202 | val_score 0.85431 | train_time  11.00 min\n",
      "epoch  13/ 50 | train_loss 0.27197 | val_loss 0.62038 | train_score 0.91650 | val_score 0.85640 | train_time  11.92 min\n",
      "epoch  14/ 50 | train_loss 0.26689 | val_loss 0.65829 | train_score 0.92242 | val_score 0.86142 | train_time  12.84 min *\n",
      "epoch  15/ 50 | train_loss 0.24739 | val_loss 0.65245 | train_score 0.92765 | val_score 0.85792 | train_time  13.75 min\n",
      "epoch  16/ 50 | train_loss 0.22436 | val_loss 0.66713 | train_score 0.93442 | val_score 0.86014 | train_time  14.67 min\n",
      "epoch  17/ 50 | train_loss 0.23189 | val_loss 0.71296 | train_score 0.93295 | val_score 0.85825 | train_time  15.58 min\n",
      "epoch  18/ 50 | train_loss 0.26104 | val_loss 0.76502 | train_score 0.92808 | val_score 0.85776 | train_time  16.50 min\n",
      "epoch  19/ 50 | train_loss 0.24738 | val_loss 0.76930 | train_score 0.93257 | val_score 0.86006 | train_time  17.42 min\n",
      "epoch  20/ 50 | train_loss 0.24243 | val_loss 0.81195 | train_score 0.93476 | val_score 0.85834 | train_time  18.33 min\n",
      "  2%|▏         | 10/500 [18:19<149:43:16, 1099.99s/trial, best loss: -0.8855087358684481]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 4.0,\n",
       " 'bias': True,\n",
       " 'early_stop': 5,\n",
       " 'hidden_dim': 150.0,\n",
       " 'mlp1_dim': 250.0,\n",
       " 'num_layers': 3.0,\n",
       " 'optimizer__lr': 0.00043522596195538234,\n",
       " 'optimizer__wd': 0,\n",
       " 'p_dropout': 0.09321689570370757,\n",
       " 'tag_embed_dim': 32,\n",
       " 'train_epochs': 50,\n",
       " 'word_dropout': 0.13314838372299523,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                           \n",
      "V2_hpo_1.0                                                                               \n",
      "Number of parameters 1817887 trainable 1817887                                           \n",
      "epoch   1/ 50 | train_loss 1.68437 | val_loss 1.66828 | train_score 0.46500 | val_score 0.47133 | train_time   1.34 min *\n",
      "epoch   2/ 50 | train_loss 1.00245 | val_loss 1.00494 | train_score 0.72039 | val_score 0.71893 | train_time   2.68 min *\n",
      "epoch   3/ 50 | train_loss 0.72008 | val_loss 0.74127 | train_score 0.79740 | val_score 0.79424 | train_time   4.02 min *\n",
      "epoch   4/ 50 | train_loss 0.59624 | val_loss 0.63569 | train_score 0.83160 | val_score 0.82323 | train_time   5.35 min *\n",
      "epoch   5/ 50 | train_loss 0.51672 | val_loss 0.57571 | train_score 0.85193 | val_score 0.83803 | train_time   6.70 min *\n",
      "epoch   6/ 50 | train_loss 0.45526 | val_loss 0.53009 | train_score 0.86798 | val_score 0.84888 | train_time   8.04 min *\n",
      "epoch   7/ 50 | train_loss 0.41114 | val_loss 0.50404 | train_score 0.87886 | val_score 0.85587 | train_time   9.37 min *\n",
      "epoch   8/ 50 | train_loss 0.36748 | val_loss 0.48550 | train_score 0.88926 | val_score 0.86203 | train_time  10.72 min *\n",
      "epoch   9/ 50 | train_loss 0.33861 | val_loss 0.47574 | train_score 0.89625 | val_score 0.86360 | train_time  12.05 min *\n",
      "epoch  10/ 50 | train_loss 0.32443 | val_loss 0.48645 | train_score 0.90066 | val_score 0.86384 | train_time  13.39 min *\n",
      "epoch  11/ 50 | train_loss 0.30015 | val_loss 0.48468 | train_score 0.90617 | val_score 0.86347 | train_time  14.73 min\n",
      "epoch  12/ 50 | train_loss 0.27476 | val_loss 0.48079 | train_score 0.91412 | val_score 0.86652 | train_time  16.07 min *\n",
      "epoch  13/ 50 | train_loss 0.26210 | val_loss 0.48834 | train_score 0.91784 | val_score 0.86717 | train_time  17.41 min *\n",
      "epoch  14/ 50 | train_loss 0.24744 | val_loss 0.50170 | train_score 0.92181 | val_score 0.86631 | train_time  18.75 min\n",
      "epoch  15/ 50 | train_loss 0.23169 | val_loss 0.50654 | train_score 0.92610 | val_score 0.86734 | train_time  20.09 min *\n",
      "epoch  16/ 50 | train_loss 0.23966 | val_loss 0.53341 | train_score 0.92448 | val_score 0.86536 | train_time  21.43 min\n",
      "epoch  17/ 50 | train_loss 0.21207 | val_loss 0.53757 | train_score 0.93308 | val_score 0.86911 | train_time  22.77 min *\n",
      "epoch  18/ 50 | train_loss 0.20312 | val_loss 0.54676 | train_score 0.93718 | val_score 0.87079 | train_time  24.11 min *\n",
      "epoch  19/ 50 | train_loss 0.19989 | val_loss 0.57199 | train_score 0.93854 | val_score 0.86952 | train_time  25.45 min\n",
      "epoch  20/ 50 | train_loss 0.19741 | val_loss 0.59374 | train_score 0.93918 | val_score 0.86738 | train_time  26.79 min\n",
      "epoch  21/ 50 | train_loss 0.18507 | val_loss 0.60051 | train_score 0.94416 | val_score 0.87034 | train_time  28.13 min\n",
      "epoch  22/ 50 | train_loss 0.20372 | val_loss 0.65226 | train_score 0.93906 | val_score 0.86277 | train_time  29.47 min\n",
      "epoch  23/ 50 | train_loss 0.20740 | val_loss 0.68622 | train_score 0.93984 | val_score 0.86331 | train_time  30.81 min\n",
      "epoch  24/ 50 | train_loss 0.21956 | val_loss 0.72082 | train_score 0.93874 | val_score 0.86343 | train_time  32.15 min\n",
      "  2%|▏         | 11/500 [50:29<183:12:23, 1348.76s/trial, best loss: -0.8855087358684481]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'p_relu',\n",
       " 'batch_size': 4.0,\n",
       " 'bias': True,\n",
       " 'early_stop': 5,\n",
       " 'hidden_dim': 250.0,\n",
       " 'mlp1_dim': 250.0,\n",
       " 'num_layers': 4.0,\n",
       " 'optimizer__lr': 0.0008501837132013326,\n",
       " 'optimizer__wd': 0,\n",
       " 'p_dropout': 0.011866820932908223,\n",
       " 'tag_embed_dim': 32,\n",
       " 'train_epochs': 50,\n",
       " 'word_dropout': 0.04474779546753615,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                           \n",
      "V2_hpo_1.0                                                                               \n",
      "Number of parameters 5932288 trainable 5932288                                           \n",
      "epoch   1/ 50 | train_loss 2.22124 | val_loss 2.22981 | train_score 0.26061 | val_score 0.25998 | train_time   1.82 min *\n",
      "epoch   2/ 50 | train_loss 0.72889 | val_loss 0.75842 | train_score 0.79298 | val_score 0.78717 | train_time   3.64 min *\n",
      "epoch   3/ 50 | train_loss 0.52873 | val_loss 0.59533 | train_score 0.84555 | val_score 0.83194 | train_time   5.45 min *\n",
      "epoch   4/ 50 | train_loss 0.41529 | val_loss 0.52302 | train_score 0.88098 | val_score 0.85538 | train_time   7.27 min *\n",
      "epoch   5/ 50 | train_loss 0.37700 | val_loss 0.55262 | train_score 0.88705 | val_score 0.85217 | train_time   9.08 min\n",
      "epoch   6/ 50 | train_loss 0.33340 | val_loss 0.58080 | train_score 0.89823 | val_score 0.84904 | train_time  10.90 min\n",
      "epoch   7/ 50 | train_loss 0.28142 | val_loss 0.55672 | train_score 0.91402 | val_score 0.85990 | train_time  12.72 min *\n",
      "epoch   8/ 50 | train_loss 0.22196 | val_loss 0.56046 | train_score 0.93067 | val_score 0.86417 | train_time  14.53 min *\n",
      "epoch   9/ 50 | train_loss 0.19914 | val_loss 0.57173 | train_score 0.93644 | val_score 0.86578 | train_time  16.35 min *\n",
      "epoch  10/ 50 | train_loss 0.17557 | val_loss 0.59292 | train_score 0.94562 | val_score 0.86869 | train_time  18.16 min *\n",
      "epoch  11/ 50 | train_loss 0.15224 | val_loss 0.62061 | train_score 0.95206 | val_score 0.87215 | train_time  19.98 min *\n",
      "epoch  12/ 50 | train_loss 0.13904 | val_loss 0.64862 | train_score 0.95566 | val_score 0.87437 | train_time  21.79 min *\n",
      "epoch  13/ 50 | train_loss 0.11114 | val_loss 0.67203 | train_score 0.96720 | val_score 0.87967 | train_time  23.61 min *\n",
      "epoch  14/ 50 | train_loss 0.13817 | val_loss 0.72095 | train_score 0.95813 | val_score 0.87079 | train_time  25.43 min\n",
      "epoch  15/ 50 | train_loss 0.09498 | val_loss 0.69632 | train_score 0.97168 | val_score 0.88189 | train_time  27.24 min *\n",
      "epoch  16/ 50 | train_loss 0.10844 | val_loss 0.76944 | train_score 0.96901 | val_score 0.87638 | train_time  29.06 min\n",
      "epoch  17/ 50 | train_loss 0.12884 | val_loss 0.83203 | train_score 0.96265 | val_score 0.87042 | train_time  30.87 min\n",
      "epoch  18/ 50 | train_loss 0.08869 | val_loss 0.80711 | train_score 0.97531 | val_score 0.87581 | train_time  32.69 min\n",
      "epoch  19/ 50 | train_loss 0.09459 | val_loss 0.81720 | train_score 0.97440 | val_score 0.87782 | train_time  34.51 min\n",
      "epoch  20/ 50 | train_loss 0.07480 | val_loss 0.80914 | train_score 0.98022 | val_score 0.88255 | train_time  36.32 min *\n",
      "epoch  21/ 50 | train_loss 0.07298 | val_loss 0.85572 | train_score 0.98096 | val_score 0.88185 | train_time  38.14 min\n",
      "epoch  22/ 50 | train_loss 0.07337 | val_loss 0.90382 | train_score 0.98131 | val_score 0.87799 | train_time  39.96 min\n",
      "epoch  23/ 50 | train_loss 0.07945 | val_loss 0.92499 | train_score 0.97950 | val_score 0.88164 | train_time  41.77 min\n",
      "epoch  24/ 50 | train_loss 0.07998 | val_loss 0.91595 | train_score 0.98022 | val_score 0.88000 | train_time  43.59 min\n",
      "epoch  25/ 50 | train_loss 0.07949 | val_loss 0.82193 | train_score 0.97905 | val_score 0.88062 | train_time  45.40 min\n",
      "epoch  26/ 50 | train_loss 0.06978 | val_loss 0.87032 | train_score 0.98137 | val_score 0.88021 | train_time  47.22 min\n",
      "  2%|▏         | 12/500 [1:37:42<243:11:54, 1794.09s/trial, best loss: -0.8855087358684481]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'gigmoid',\n",
       " 'batch_size': 4.0,\n",
       " 'bias': True,\n",
       " 'early_stop': 5,\n",
       " 'hidden_dim': 100.0,\n",
       " 'mlp1_dim': 150.0,\n",
       " 'num_layers': 4.0,\n",
       " 'optimizer__lr': 0.0002356003389179894,\n",
       " 'optimizer__wd': 0,\n",
       " 'p_dropout': 0.06131302393454804,\n",
       " 'tag_embed_dim': 32,\n",
       " 'train_epochs': 50,\n",
       " 'word_dropout': 0.1032776251369739,\n",
       " 'word_embed_dim': 300}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version:                                                                             \n",
      "V2_hpo_1.0                                                                                 \n",
      "Number of parameters 1133987 trainable 1133987                                             \n",
      "  2%|▏         | 12/500 [1:38:11<66:32:58, 490.94s/trial, best loss: -0.8855087358684481]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3d320fd4212b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0mmax_queue_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m              max_evals=iters)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a1a13dffa3b2>\u001b[0m in \u001b[0;36minit_objective\u001b[0;34m(space, save)\u001b[0m\n\u001b[1;32m     66\u001b[0m                           \u001b[0mepochs_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                           \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'early_stop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                           save=save)\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Neural_Networks_for_Dependency_Parser/src/pytorch_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, device, train_dataset, val_dataset, train_epochs, batch_size, optimizer_params, prints, p_dropout, epochs_save, lr_decay, early_stop, save)\u001b[0m\n\u001b[1;32m    497\u001b[0m                         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_run_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Neural_Networks_for_Dependency_Parser/src/pytorch_utils.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, device, data_loader, train, results, decision_func)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp_hw2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iters = 500\n",
    "\n",
    "_ = hpo.fmin(init_objective,\n",
    "             init_space,\n",
    "             algo=hpo.tpe.suggest,\n",
    "             trials=init_trials,\n",
    "             max_queue_len=1,\n",
    "             max_evals=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "    dill.dump(init_trials, f)\n",
    "init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'activation': 'tanh',\n",
    " 'batch_size': 4.0,\n",
    " 'bias': True,\n",
    " 'hidden_dim': 300.0,\n",
    " 'mlp1_dim': 400.0,\n",
    " 'num_layers': 3.0,\n",
    " 'optimizer__lr': 0.0001461425145455605,\n",
    " 'optimizer__wd': 4.203613249902623e-07,\n",
    " 'p_dropout': 0.19840346306446244,\n",
    " 'tag_embed_dim': 36.0,\n",
    " 'train_epochs': 10,\n",
    " 'word_dropout': 0.29891514439839717,\n",
    " 'word_embed_dim': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ptu.load_model(version=version, versions_dir=versions_dir, epoch='best', seed=42)\n",
    "# loss, score = checkpoint.predict(test_dataset.dataset,\n",
    "#                                  batch_size=32,\n",
    "#                                  device=device,\n",
    "#                                  results=False,\n",
    "#                                  decision_func=chu.test_chu_liu_edmonds)\n",
    "# print(f'chu_liu_edmonds_UAS: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# checkpoint.model = checkpoint.model.to(device)\n",
    "# checkpoint.model.train()\n",
    "# batch_size = 32\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset=train_dataset.dataset, batch_size=batch_size, shuffle=True)\n",
    "# for batch in loader:\n",
    "#     loss, flat_y, flat_out, mask, out, y = utils.loss_decision_func(checkpoint, device, batch, prints=True)\n",
    "#     break\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw2",
   "language": "python",
   "name": "nlp_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
