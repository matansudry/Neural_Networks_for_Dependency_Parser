{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "# import dill\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src import utils\n",
    "from src import bilstm\n",
    "import src.dataset as dset\n",
    "import src.pytorch_utils as ptu\n",
    "import src.chu_liu_edmonds as chu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "versions_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125430/125430 [00:16<00:00, 7426.27it/s]\n",
      "100%|██████████| 25325/25325 [00:03<00:00, 7434.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dset.DataSet('data/train.labeled', tqdm_bar=True, use_glove=True)\n",
    "test_dataset = dset.DataSet('data/test.labeled', train_dataset=train_dataset, tqdm_bar=True, use_glove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'V2_1.10'\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model version: V2_1.10\n",
      "Number of parameters 8144200 trainable 3894400\n"
     ]
    }
   ],
   "source": [
    "model = bilstm.BiLSTM(train_dataset=train_dataset,\n",
    "                      word_embed_dim=300,\n",
    "                      tag_embed_dim=25,\n",
    "                      hidden_dim=200,\n",
    "                      num_layers=4,\n",
    "                      bias=True,\n",
    "                      lstm_activation=None,\n",
    "                      p_dropout=0.3,\n",
    "                      attention=utils.MultiplicativeAttention,\n",
    "                      softmax=nn.LogSoftmax(dim=2),\n",
    "                      glove=True,\n",
    "                      freeze=True)\n",
    "\n",
    "checkpoint = ptu.Checkpoint(versions_dir=versions_dir,\n",
    "                            version=version,\n",
    "                            model=model,\n",
    "                            score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "                            loss_decision_func=utils.loss_decision_func,\n",
    "                            out_decision_func=chu.test_chu_liu_edmonds,\n",
    "                            seed=42,\n",
    "                            optimizer=torch.optim.AdamW,\n",
    "                            criterion=nn.NLLLoss,\n",
    "                            save=save,\n",
    "                            prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ptu.load_model(version=version, versions_dir=versions_dir, epoch=40, seed=42)\n",
    "# display(checkpoint.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 20 | train_loss 0.44128 | val_loss 0.47693 | train_score 0.86718 | val_score 0.85866 | train_time   1.07 min *\n",
      "epoch   2/ 20 | train_loss 0.31957 | val_loss 0.41053 | train_score 0.90104 | val_score 0.87630 | train_time   2.91 min *\n",
      "epoch   3/ 20 | train_loss 0.22960 | val_loss 0.37935 | train_score 0.92700 | val_score 0.88658 | train_time   4.56 min *\n",
      "epoch   4/ 20 | train_loss 0.19850 | val_loss 0.39567 | train_score 0.93610 | val_score 0.88559 | train_time   6.28 min\n",
      "epoch   5/ 20 | train_loss 0.15092 | val_loss 0.39126 | train_score 0.94990 | val_score 0.89184 | train_time   7.52 min *\n",
      "epoch   6/ 20 | train_loss 0.11670 | val_loss 0.41072 | train_score 0.96096 | val_score 0.89340 | train_time   9.53 min *\n",
      "epoch   7/ 20 | train_loss 0.09383 | val_loss 0.43074 | train_score 0.96850 | val_score 0.89595 | train_time  11.23 min *\n",
      "epoch   8/ 20 | train_loss 0.08163 | val_loss 0.45031 | train_score 0.97336 | val_score 0.89702 | train_time  12.97 min *\n",
      "epoch   9/ 20 | train_loss 0.06424 | val_loss 0.46356 | train_score 0.97804 | val_score 0.90068 | train_time  14.81 min *\n",
      "epoch  10/ 20 | train_loss 0.06242 | val_loss 0.47661 | train_score 0.97835 | val_score 0.89994 | train_time  16.55 min\n",
      "epoch  11/ 20 | train_loss 0.05030 | val_loss 0.47421 | train_score 0.98338 | val_score 0.90232 | train_time  18.19 min *\n",
      "epoch  12/ 20 | train_loss 0.04675 | val_loss 0.51313 | train_score 0.98443 | val_score 0.90142 | train_time  19.81 min\n",
      "epoch  13/ 20 | train_loss 0.04848 | val_loss 0.52471 | train_score 0.98440 | val_score 0.90088 | train_time  21.09 min\n",
      "epoch  14/ 20 | train_loss 0.04408 | val_loss 0.52431 | train_score 0.98499 | val_score 0.90709 | train_time  22.29 min *\n",
      "epoch  15/ 20 | train_loss 0.03223 | val_loss 0.53260 | train_score 0.98879 | val_score 0.90458 | train_time  24.29 min\n",
      "epoch  16/ 20 | train_loss 0.03611 | val_loss 0.54825 | train_score 0.98821 | val_score 0.90425 | train_time  26.00 min\n",
      "epoch  17/ 20 | train_loss 0.02671 | val_loss 0.58408 | train_score 0.99122 | val_score 0.90249 | train_time  27.24 min\n",
      "epoch  18/ 20 | train_loss 0.02728 | val_loss 0.56267 | train_score 0.99101 | val_score 0.90697 | train_time  28.51 min\n",
      "epoch  19/ 20 | train_loss 0.02507 | val_loss 0.59075 | train_score 0.99205 | val_score 0.90479 | train_time  29.72 min\n",
      "epoch  20/ 20 | train_loss 0.02390 | val_loss 0.56683 | train_score 0.99209 | val_score 0.90943 | train_time  30.93 min *\n",
      "epoch  21/ 40 | train_loss 0.02401 | val_loss 0.57217 | train_score 0.99219 | val_score 0.90713 | train_time  32.00 min\n",
      "epoch  22/ 40 | train_loss 0.01698 | val_loss 0.61539 | train_score 0.99438 | val_score 0.90684 | train_time  33.24 min\n",
      "epoch  23/ 40 | train_loss 0.00894 | val_loss 0.65521 | train_score 0.99687 | val_score 0.90972 | train_time  34.50 min *\n",
      "epoch  24/ 40 | train_loss 0.00681 | val_loss 0.70134 | train_score 0.99778 | val_score 0.91063 | train_time  36.16 min *\n",
      "epoch  25/ 40 | train_loss 0.00439 | val_loss 0.72958 | train_score 0.99851 | val_score 0.91050 | train_time  38.00 min\n",
      "epoch  26/ 40 | train_loss 0.00296 | val_loss 0.72202 | train_score 0.99890 | val_score 0.91112 | train_time  39.66 min *\n",
      "epoch  27/ 40 | train_loss 0.00240 | val_loss 0.75018 | train_score 0.99907 | val_score 0.91248 | train_time  41.28 min *\n",
      "epoch  28/ 40 | train_loss 0.00183 | val_loss 0.76114 | train_score 0.99929 | val_score 0.91264 | train_time  42.88 min *\n",
      "epoch  29/ 40 | train_loss 0.00173 | val_loss 0.78610 | train_score 0.99929 | val_score 0.91244 | train_time  44.51 min\n",
      "epoch  30/ 40 | train_loss 0.00162 | val_loss 0.79183 | train_score 0.99927 | val_score 0.91379 | train_time  45.72 min *\n",
      "epoch  31/ 40 | train_loss 0.00136 | val_loss 0.79404 | train_score 0.99935 | val_score 0.91375 | train_time  47.70 min\n",
      "epoch  32/ 40 | train_loss 0.00124 | val_loss 0.81100 | train_score 0.99944 | val_score 0.91289 | train_time  48.90 min\n",
      "epoch  33/ 40 | train_loss 0.00114 | val_loss 0.81957 | train_score 0.99948 | val_score 0.91346 | train_time  50.08 min\n",
      "epoch  34/ 40 | train_loss 0.00112 | val_loss 0.82752 | train_score 0.99951 | val_score 0.91363 | train_time  51.34 min\n",
      "epoch  35/ 40 | train_loss 0.00105 | val_loss 0.82885 | train_score 0.99956 | val_score 0.91408 | train_time  52.53 min *\n",
      "epoch  36/ 40 | train_loss 0.00103 | val_loss 0.83605 | train_score 0.99952 | val_score 0.91350 | train_time  54.56 min\n",
      "epoch  37/ 40 | train_loss 0.00101 | val_loss 0.83749 | train_score 0.99952 | val_score 0.91330 | train_time  55.74 min\n",
      "epoch  38/ 40 | train_loss 0.00099 | val_loss 0.84070 | train_score 0.99953 | val_score 0.91346 | train_time  56.94 min\n",
      "epoch  39/ 40 | train_loss 0.00097 | val_loss 0.84302 | train_score 0.99953 | val_score 0.91326 | train_time  58.15 min\n",
      "epoch  40/ 40 | train_loss 0.00097 | val_loss 0.84508 | train_score 0.99952 | val_score 0.91313 | train_time  59.39 min\n",
      "epoch  41/ 60 | train_loss 0.00165 | val_loss 0.85121 | train_score 0.99933 | val_score 0.91231 | train_time  60.45 min\n",
      "epoch  42/ 60 | train_loss 0.00147 | val_loss 0.86156 | train_score 0.99943 | val_score 0.91141 | train_time  61.68 min\n",
      "epoch  43/ 60 | train_loss 0.00163 | val_loss 0.85658 | train_score 0.99935 | val_score 0.91268 | train_time  62.89 min\n",
      "epoch  44/ 60 | train_loss 0.00134 | val_loss 0.85444 | train_score 0.99945 | val_score 0.91289 | train_time  64.08 min\n",
      "epoch  45/ 60 | train_loss 0.00129 | val_loss 0.86417 | train_score 0.99947 | val_score 0.91141 | train_time  65.35 min\n",
      "epoch  46/ 60 | train_loss 0.00127 | val_loss 0.88546 | train_score 0.99951 | val_score 0.91170 | train_time  66.98 min\n",
      "epoch  47/ 60 | train_loss 0.00133 | val_loss 0.89287 | train_score 0.99948 | val_score 0.91301 | train_time  68.32 min\n",
      "epoch  48/ 60 | train_loss 0.00127 | val_loss 0.88473 | train_score 0.99953 | val_score 0.91137 | train_time  69.52 min\n",
      "epoch  49/ 60 | train_loss 0.00116 | val_loss 0.88122 | train_score 0.99954 | val_score 0.91248 | train_time  70.71 min\n",
      "epoch  50/ 60 | train_loss 0.00110 | val_loss 0.89403 | train_score 0.99952 | val_score 0.91207 | train_time  72.02 min\n",
      "epoch  51/ 60 | train_loss 0.00097 | val_loss 0.89755 | train_score 0.99961 | val_score 0.91227 | train_time  73.82 min\n",
      "epoch  52/ 60 | train_loss 0.00095 | val_loss 0.91120 | train_score 0.99963 | val_score 0.91297 | train_time  75.02 min\n",
      "epoch  53/ 60 | train_loss 0.00094 | val_loss 0.89205 | train_score 0.99960 | val_score 0.91256 | train_time  76.27 min\n",
      "epoch  54/ 60 | train_loss 0.00093 | val_loss 0.90681 | train_score 0.99959 | val_score 0.91297 | train_time  77.48 min\n",
      "epoch  55/ 60 | train_loss 0.00088 | val_loss 0.90477 | train_score 0.99965 | val_score 0.91293 | train_time  78.97 min\n",
      "epoch  56/ 60 | train_loss 0.00113 | val_loss 0.90920 | train_score 0.99954 | val_score 0.91433 | train_time  80.59 min *\n",
      "epoch  57/ 60 | train_loss 0.00115 | val_loss 0.91410 | train_score 0.99963 | val_score 0.91346 | train_time  82.31 min\n",
      "epoch  58/ 60 | train_loss 0.00090 | val_loss 0.90931 | train_score 0.99965 | val_score 0.91379 | train_time  83.56 min\n",
      "epoch  59/ 60 | train_loss 0.00085 | val_loss 0.92087 | train_score 0.99966 | val_score 0.91178 | train_time  84.81 min\n",
      "epoch  60/ 60 | train_loss 0.00088 | val_loss 0.94495 | train_score 0.99965 | val_score 0.91170 | train_time  85.99 min\n"
     ]
    }
   ],
   "source": [
    "word_dropout_alpha = 0.25\n",
    "hyperparam_list = [\n",
    "    {'train_epochs': 20, 'batch_size': 8, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 5e-7}},\n",
    "    {'train_epochs': 20, 'batch_size': 8, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1e-6}, 'lr_decay': 0.2},\n",
    "    {'train_epochs': 20, 'batch_size': 8, 'optimizer_params': {'lr': 4e-4, 'weight_decay': 0.0}},  # 1e-6\n",
    "]\n",
    "\n",
    "for session in hyperparam_list:\n",
    "    checkpoint.train(device=device,\n",
    "                     train_dataset=train_dataset.dataset(word_dropout_alpha, train=True),\n",
    "                     val_dataset=test_dataset.dataset(train=False),\n",
    "                     prints=True,\n",
    "                     epochs_save=5,\n",
    "                     save=save,\n",
    "#                      early_stop=5,\n",
    "                     **session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version = 'V2_hpo_1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions = {\n",
    "#     'Additive': utils.AdditiveAttention,\n",
    "#     'Multiplicative': utils.MultiplicativeAttention,\n",
    "# }\n",
    "\n",
    "# softmaxs = {\n",
    "#     'LogSoftmax': nn.LogSoftmax(dim=2),\n",
    "# #     'Softmax': nn.Softmax(dim=2),\n",
    "# }\n",
    "\n",
    "# activations = dict(sorted(list({\n",
    "#     'tanh': nn.Tanh(),\n",
    "#     'hard_tanh': nn.Hardtanh(),\n",
    "# #     'relu': nn.ReLU(),\n",
    "# #     'elu': nn.ELU(),\n",
    "# #     'leaky_relu': nn.LeakyReLU(),\n",
    "#     'p_relu': nn.PReLU(),\n",
    "# #     'relu6': nn.ReLU6(),\n",
    "# #     'gelu': nn.GELU(),\n",
    "# #     'sigmoid': nn.Sigmoid(),\n",
    "# }.items()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hyperopt as hpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_space = dict(sorted(list({\n",
    "# #     'train_epochs': 50,\n",
    "#     'batch_size': 16, #hpo.hp.quniform('batch_size', low=4, high=5, q=1),  # 16-32-64\n",
    "#     'optimizer__lr': hpo.hp.uniform('optimizer__lr', low=8e-4, high=2e-3),\n",
    "#     'optimizer__wd': hpo.hp.uniform('optimizer__wd', low=5e-7, high=5e-6),# 0.0\n",
    "# #     'early_stop': 5,\n",
    "    \n",
    "#     'word_embed_dim': 300,  # 300\n",
    "#     'tag_embed_dim': 32, #hpo.hp.quniform('tag_embed_dim', low=30, high=50, q=4), #25\n",
    "#     'hidden_dim': hpo.hp.quniform('hidden_dim', low=200, high=300, q=50), #125,  # \n",
    "#     'num_layers': hpo.hp.quniform('num_layers', low=3, high=4, q=1),#2,  # \n",
    "#     'bias': True, #hpo.hp.choice('bias', [True, False]),\n",
    "#     'attention_dim': hpo.hp.quniform('attention_dim', low=200, high=300, q=50),#100,  # \n",
    "#     'attention': hpo.hp.choice('attention', list(attentions.keys())),\n",
    "#     'activation': hpo.hp.choice('activation', list(activations.keys())),\n",
    "#     'softmax': hpo.hp.choice('softmax', list(softmaxs.keys())),\n",
    "#     'p_dropout': hpo.hp.uniform('p_dropout', low=0.3, high=0.6),#0.1,  # \n",
    "#     'lr_decay': hpo.hp.uniform('lr_decay', low=0.15, high=0.25),#0.1,  # \n",
    "#     'freeze': True, #hpo.hp.choice('freeze', [True, False]),\n",
    "# }.items()), key=lambda x: x[0]))\n",
    "\n",
    "# def init_objective(space, save=False):\n",
    "#     display(space)\n",
    "#     last_score = init_log['test_score'].max() if len(init_log) > 0 else 0.0\n",
    "#     batch_size = int(2 ** space['batch_size'])\n",
    "# #     attention = utils.MultiplicativeAttention if space['attention'] == 'Multiplicative' else utils.AdditiveAttention\n",
    "# #     activation = space['attention'] if space['attention'] != 'Multiplicative' else 'tanh'\n",
    "# #     activation = activations[activation]\n",
    "    \n",
    "#     model = m2.Model2(train_dataset=train_dataset,\n",
    "#                       word_embed_dim=space['word_embed_dim'],  # 300\n",
    "#                       tag_embed_dim=space['tag_embed_dim'],  # 32\n",
    "#                       hidden_dim=int(space['hidden_dim']),  # 125\n",
    "#                       num_layers=int(space['num_layers']),  # 2\n",
    "#                       bias=space['bias'],  # True\n",
    "#                       attention_dim=int(space['attention_dim']),  # 10\n",
    "#                       activation=activations[space['activation']],\n",
    "#                       p_dropout=space['p_dropout'],  # 0.5\n",
    "#                       attention=attentions[space['attention']],\n",
    "#                       softmax=softmaxs[space['softmax']],\n",
    "#                       glove=True,\n",
    "#                       freeze=space['freeze'])\n",
    "\n",
    "#     init_checkpoint = ptu.Checkpoint(versions_dir=versions_dir,\n",
    "#                                      version=version,\n",
    "#                                      model=model,\n",
    "#                                      score=lambda y_true, y_pred: (np.array(y_true) == np.array(y_pred)).mean(),\n",
    "#                                      loss_decision_func=utils.loss_decision_func,\n",
    "#                                      out_decision_func=lambda y_pred, flat_y_pred, mask, padding: flat_y_pred.argmax(axis=1),\n",
    "#                                      seed=42,\n",
    "#                                      optimizer=torch.optim.AdamW,\n",
    "#                                      criterion=nn.NLLLoss,\n",
    "#                                      save=False,\n",
    "#                                      prints=True)\n",
    "    \n",
    "#     word_dropout_alpha = 0.25\n",
    "#     hyperparam_list = [\n",
    "# #         {'train_epochs': 1, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': 5e-7}},\n",
    "#         {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': 5e-7}},\n",
    "#         {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': space['optimizer__wd']}, 'lr_decay': space['lr_decay']},\n",
    "# #         {'train_epochs': 20, 'batch_size': 16, 'optimizer_params': {'lr': space['optimizer__lr'], 'weight_decay': space['optimizer__wd']}, 'lr_decay': space['lr_decay']},\n",
    "#     #     {'train_epochs': 20, 'batch_size': 32, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1.5e-6}, 'lr_decay': 0.2},\n",
    "#     #     {'train_epochs': 20, 'batch_size': 64, 'optimizer_params': {'lr': 2e-3, 'weight_decay': 1.5e-6}, 'lr_decay': 0.2},\n",
    "#     ]\n",
    "\n",
    "#     for session in hyperparam_list:\n",
    "#         init_checkpoint.train(device=device,\n",
    "#                               train_dataset=train_dataset.dataset(word_dropout_alpha, train=True),\n",
    "#                               val_dataset=test_dataset.dataset(train=False),\n",
    "#                               prints=True,\n",
    "#                               epochs_save=5,\n",
    "#                               save=save,\n",
    "#         #                       early_stop=5,\n",
    "#                               **session)    \n",
    "    \n",
    "#     train_score = init_checkpoint.get_log(col='train_score', epoch=-1)\n",
    "#     test_score = init_checkpoint.get_log(col='val_score', epoch=-1)\n",
    "# #     print('test_score', test_score)\n",
    "#     ###############################################################\n",
    "#     if test_score > last_score:\n",
    "#         init_checkpoint.save(epoch=True)\n",
    "#     init_log.loc[init_log.index.max() + 1 if len(init_log) > 0 else 0] = [time.strftime('%d-%m-%Y %H:%M:%S'),\n",
    "# #                                                                           train_score,\n",
    "#                                                                           test_score,\n",
    "#                                                                           space] + list(space.values())\n",
    "    \n",
    "#     with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "#         dill.dump(init_trials, f)\n",
    "#     init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)\n",
    "\n",
    "#     return -test_score\n",
    "\n",
    "# # session_space = dict(sorted(list({\n",
    "# #     'train_epochs': 5,\n",
    "# #     'batch_size_mult': min(len(X_train), int(2**hpo.hp.quniform('batch_size_mult', low=5, high=9, q=1))),\n",
    "# #     'optimizer__lr_mult': hpo.hp.uniform('optimizer__lr_mult', low=1e-5, high=1e-3),\n",
    "# #     'optimizer__weight_decay': hpo.hp.uniform('optimizer__weight_decay', low=1e-5, high=1e-3),\n",
    "# #     'p_dropout': max(0.0, min(0.9, hpo.hp.normal('p_dropout', mu=0.5, sigma=0.15))),\n",
    "# # }.items()), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init_trials = hpo.Trials()\n",
    "# # init_log = pd.DataFrame(columns=['timestamp',\n",
    "# #                                  # 'train_score',\n",
    "# #                                  'test_score',\n",
    "# #                                  'space'] + list(init_space.keys()))\n",
    "\n",
    "# # with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "# #     dill.dump(init_trials, f)\n",
    "# # init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)\n",
    "\n",
    "# with open(os.path.join(versions_dir, version, 'trials.pth'), \"rb\") as f:\n",
    "#     init_trials = dill.load(f)\n",
    "# init_log = pd.read_csv(os.path.join(versions_dir, version, 'trials_log.csv'))\n",
    "# display(init_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iters = 500\n",
    "\n",
    "# _ = hpo.fmin(init_objective,\n",
    "#              init_space,\n",
    "#              algo=hpo.tpe.suggest,\n",
    "#              trials=init_trials,\n",
    "#              max_queue_len=1,\n",
    "#              max_evals=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(versions_dir, version, 'trials.pth'), 'wb') as f:\n",
    "#     dill.dump(init_trials, f)\n",
    "# init_log.to_csv(os.path.join(versions_dir, version, 'trials_log.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ptu.load_model(version=version, versions_dir=versions_dir, epoch='best', seed=42)\n",
    "# loss, score = checkpoint.predict(test_dataset.dataset,\n",
    "#                                  batch_size=32,\n",
    "#                                  device=device,\n",
    "#                                  results=False,\n",
    "#                                  decision_func=chu.test_chu_liu_edmonds)\n",
    "# print(f'chu_liu_edmonds_UAS: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# checkpoint.model = checkpoint.model.to(device)\n",
    "# checkpoint.model.train()\n",
    "# batch_size = 32\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset=train_dataset.dataset, batch_size=batch_size, shuffle=True)\n",
    "# for batch in loader:\n",
    "#     loss, flat_y, flat_out, mask, out, y = utils.loss_decision_func(checkpoint, device, batch, prints=True)\n",
    "#     break\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw2",
   "language": "python",
   "name": "nlp_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
